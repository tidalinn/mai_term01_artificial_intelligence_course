{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<br>\n",
    "<div class=\"toc\">\n",
    "    <ul class=\"toc-item\">\n",
    "        <li>\n",
    "            <span>\n",
    "                <a href=\"#1.-Импорт-библиотек\">\n",
    "                    <span class=\"toc-item-num\">1.&nbsp;&nbsp;</span>\n",
    "                    Импорт библиотек\n",
    "                </a>\n",
    "            </span>\n",
    "        </li>\n",
    "        <li>\n",
    "            <span>\n",
    "                <a href=\"#2.-Подготовка-данных\">\n",
    "                    <span class=\"toc-item-num\">2.&nbsp;&nbsp;</span>\n",
    "                    Подготовка данных\n",
    "                </a>\n",
    "            </span>\n",
    "        </li>\n",
    "        <li style=\"margin-top: 10px\">\n",
    "            <span>\n",
    "                <a href=\"#3.-Алгоритмы-градиентного-спуска\">\n",
    "                    <span class=\"toc-item-num\">3.&nbsp;&nbsp;</span>\n",
    "                    Алгоритмы градиентного спуска\n",
    "                </a>\n",
    "            </span>\n",
    "            <ul class=\"toc-item\">\n",
    "                <li>\n",
    "                    <span>\n",
    "                        <a href=\"#3.1.-Математическое-обоснование\">\n",
    "                            <span class=\"toc-item-num\">3.1.&nbsp;&nbsp;</span>\n",
    "                            Математическое обоснование\n",
    "                        </a>\n",
    "                    </span>\n",
    "                </li>\n",
    "                <li>\n",
    "                    <span>\n",
    "                        <a href=\"#3.2.-Реализация-алгоритмов\">\n",
    "                            <span class=\"toc-item-num\">3.2.&nbsp;&nbsp;</span>\n",
    "                            Реализация алгоритмов\n",
    "                        </a>\n",
    "                    </span>\n",
    "                    <ul class=\"toc-item\">\n",
    "                        <li>\n",
    "                            <span>\n",
    "                                <a href=\"#3.2.1.-Градиентный-спуск\">\n",
    "                                    <span class=\"toc-item-num\">3.2.1.&nbsp;&nbsp;</span>\n",
    "                                    Градиентный спуск\n",
    "                                </a>\n",
    "                            </span>\n",
    "                        </li>\n",
    "                        <li>\n",
    "                            <span>\n",
    "                                <a href=\"#3.2.2.-Стохастический-градиентный-спуск\">\n",
    "                                    <span class=\"toc-item-num\">3.2.2.&nbsp;&nbsp;</span>\n",
    "                                    Стохастический градиентный спуск\n",
    "                                </a>\n",
    "                            </span>\n",
    "                        </li>\n",
    "                        <li>\n",
    "                            <span>\n",
    "                                <a href=\"#3.2.3.-Стохастический-градиентный-спуск-по-мини-батчам\">\n",
    "                                    <span class=\"toc-item-num\">3.2.3.&nbsp;&nbsp;</span>\n",
    "                                    Стохастический градиентный спуск по мини-батчам\n",
    "                                </a>\n",
    "                            </span>\n",
    "                        </li>\n",
    "                    </ul>\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li style=\"margin-top: 10px\">\n",
    "            <span>\n",
    "                <a href=\"#4.-Алгоритмы-оптимизации\">\n",
    "                    <span class=\"toc-item-num\">4.&nbsp;&nbsp;</span>\n",
    "                    Алгоритмы оптимизации\n",
    "                </a>\n",
    "            </span>\n",
    "            <ul>\n",
    "                <li>\n",
    "                    <span>\n",
    "                        <a href=\"#4.1.-Математическое-обоснование\">\n",
    "                            <span class=\"toc-item-num\">4.1.&nbsp;&nbsp;</span>\n",
    "                            Математическое обоснование\n",
    "                        </a>\n",
    "                    </span>\n",
    "                </li>\n",
    "                <li>\n",
    "                    <span>\n",
    "                        <a href=\"#4.2.-Реализация-алгоритмов\">\n",
    "                            <span class=\"toc-item-num\">4.2.&nbsp;&nbsp;</span>\n",
    "                            Реализация алгоритмов\n",
    "                        </a>\n",
    "                    </span>\n",
    "                    <ul class=\"toc-item\">\n",
    "                        <li>\n",
    "                            <span>\n",
    "                                <a href=\"#4.2.1.-ADAM\">\n",
    "                                    <span class=\"toc-item-num\">4.2.1.&nbsp;&nbsp;</span>\n",
    "                                    ADAM\n",
    "                                </a>\n",
    "                            </span>\n",
    "                        </li>\n",
    "                        <li>\n",
    "                            <span>\n",
    "                                <a href=\"#4.2.2.-Метод-моментов\">\n",
    "                                    <span class=\"toc-item-num\">4.2.2.&nbsp;&nbsp;</span>\n",
    "                                    Метод моментов\n",
    "                                </a>\n",
    "                            </span>\n",
    "                        </li>\n",
    "                    </ul>\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li style=\"margin-top: 10px\">\n",
    "            <span>\n",
    "                <a href=\"#5.-Искусственные-ландшафты\">\n",
    "                    <span class=\"toc-item-num\">5.&nbsp;&nbsp;</span>\n",
    "                    Искусственные ландшафты\n",
    "                </a>\n",
    "            </span>\n",
    "            <ul>\n",
    "                <li>\n",
    "                    <span>\n",
    "                        <a href=\"#5.1.-Функция-МакКормика\">\n",
    "                            <span class=\"toc-item-num\">5.1.&nbsp;&nbsp;</span>\n",
    "                            Функция МакКормика\n",
    "                        </a>\n",
    "                    </span>\n",
    "                </li>\n",
    "                <li>\n",
    "                    <span>\n",
    "                        <a href=\"#5.2.-Функция-Изома\">\n",
    "                            <span class=\"toc-item-num\">5.2.&nbsp;&nbsp;</span>\n",
    "                            Функция Изома\n",
    "                        </a>\n",
    "                    </span>\n",
    "                </li>\n",
    "                <li>\n",
    "                    <span>\n",
    "                        <a href=\"#5.3.-Функция-Растригина\">\n",
    "                            <span class=\"toc-item-num\">5.3.&nbsp;&nbsp;</span>\n",
    "                            Функция Растригина\n",
    "                        </a>\n",
    "                    </span>\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li style=\"margin-top: 10px\">\n",
    "            <span>\n",
    "                <a href=\"#6.-Общий-вывод\">\n",
    "                    <span class=\"toc-item-num\">6.&nbsp;&nbsp;</span>\n",
    "                    Общий вывод\n",
    "                </a>\n",
    "            </span>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа №1: Stochastic Gradient Descend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача 1:** запрограммировать собственную реализацию Стохастического градиентного спуска.\n",
    "\n",
    "**Задача 2:** запрограммировать реализацию моментной и адаптивной модификаций Стохастического градиентного спуска.\n",
    "\n",
    "**Тестирование:** протестировать работу алгоритмов на тестовых искусственных ландшафтах, взятых со страницы: [Тестовые функции для оптимизации](https://ru.wikipedia.org/wiki/Тестовые_функции_для_оптимизации).\n",
    "\n",
    "**Источники данных:** 1) набор данных взят с платформы [Kaggle](https://www.kaggle.com/datasets/rounakbanik/pokemon); 2) сгенерированный набор точек.\n",
    "\n",
    "**Описание данных:** набор данных содержит информацию о семи поколениях Покемонов.\n",
    "\n",
    "---\n",
    "\n",
    "Для реализации поставленных задач из набора данных `datasets/pokemon.csv` будут взяты следующие столбцы:\n",
    "\n",
    "* `defense` - признак объектов\n",
    "* `attack` - целевой признак"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Импорт библиотек"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Установка необходимых библиотек:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sympy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импорт всех необходимых библиотек:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "\n",
    "from typing import Tuple, Mapping\n",
    "\n",
    "from sympy.abc import x, y\n",
    "from sympy import diff, lambdify\n",
    "from sympy import cos, sin, exp, pi\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from scipy import optimize\n",
    "\n",
    "from utils.plot_charts import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Внутрипроектный модуль `plot_charts` включает в себя следующие функции:\n",
    "\n",
    "* `plot_scatter` - построение двумерного статичного графика распределения значений\n",
    "\n",
    "* `plot_gradient_descent_loss_2d` - построение двумерного графика уровней функций потерь\n",
    "\n",
    "* `plot_gradient_descent_function_2d` - построение двумерного графика уровней заданной функции\n",
    "\n",
    "* `plot_function_2d` - построение двумерного статичного графика функции\n",
    "\n",
    "* `plot_gradient_descent_3d` - построение трёхмерного статичного графика как без, так и с градиентным спуском\n",
    "\n",
    "* `plot_function_gradient_2d_3d` - построение двумерного и трёхмерного статичных графиков в один ряд"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Подготовка данных "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Набор данных Pokemon**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение набора данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (pd.read_csv('datasets/pokemon.csv', usecols=['defense', 'attack'], index_col=0)\n",
    "          .reset_index())[['defense', 'attack']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран полученного набора данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение значений для `Х` и `Y` из набора данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pokemon = StandardScaler().fit_transform(data[['defense']]).flatten()\n",
    "y_pokemon = data['attack'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика распределения данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(x_pokemon, y_pokemon, 'Распределение X по Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сгенерированные точки**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание множества точек:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_linear = np.linspace(-5, 5, 1000)\n",
    "y_linear = np.linspace(-5, 5, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика распределения данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(x_linear, y_linear, 'Распределение X по Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Вывод**\n",
    ">\n",
    ">Данные подготовлены для реализации алгоритма."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Алгоритмы градиентного спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Математическое обоснование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Градиентный спуск**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итеративный алгоритм поиска минимума функции потерь. Иначе говоря, способ нахождения локального минимума функции потерь в процессе движения в направлении антиградиента.\n",
    "\n",
    "> **Градиент** $ \\nabla f(x) $ показывает направление самого быстого роста функции.\n",
    ">\n",
    "> **Антиградиент** $ - \\nabla f(x) $ показывает направление наискорейшего убывания функции.\n",
    ">\n",
    "> **Градиент векторной функции** - вектор, состоящий из производных ответа по каждому аргументу.\n",
    "\n",
    "$$ \\nabla f(x) = \\Big( \\frac{\\delta f}{\\delta x_1}, \\frac{\\delta f}{\\delta x_2} ... \\frac{\\delta f}{\\delta x_n} \\Big) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Алгоритм:**\n",
    ">\n",
    "> 1. В аргументах алгоритма задаётся начальное значение параметра.\n",
    "> 2. Рассчитывается градиент функции потерь.\n",
    "> 3. Вычисляется новое значение параметра.\n",
    "\n",
    "Алгоритм градиентного спуска завершается, если выполнено хотя бы одно из условий:\n",
    "\n",
    "* Алгоритм прошёл заданное количество итераций.\n",
    "* Размер шага становится меньше заданного порога."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Градиентный спуск по матрице признаков**\n",
    "\n",
    "> **Функция потерь** $L(y, \\hat y)$ - возвращает число потерь от неправильных ответов модели.\n",
    "\n",
    "$$ L(y, x) = (y - \\hat y)^2 = (Y - X w)^2 $$\n",
    "\n",
    "* $ y $ - вектор целевого признака\n",
    "* $ \\hat y $ - вектор предсказаний\n",
    "* $ X $ - матрица признаков (нулевой столбец состоит из единиц)\n",
    "* $ w $ - вектор весов линейной регрессии\n",
    "\n",
    "Распространённой функцией потерь линейной регрессии является **среднеквадратическая ошибка**:\n",
    "\n",
    "$$ MSE = \\frac {1}{n} \\sum_{i=1}^n (y_i - \\hat y_i)^2 = \\frac {1}{n} (Y - X w)^T (Y - X w) $$\n",
    "\n",
    "$$ MSE = \\frac {1}{n} (Y^T Y - w^T X^T Y - Y^T X w + w^T X^T X w) $$\n",
    "\n",
    "$$ \\Big[ w^T X^T Y = (Y^T X w)^T \\Big] $$\n",
    "\n",
    "$$ MSE = \\frac {1}{n} (Y^T Y - 2 w^T X^T Y + w^T X^T X w) $$\n",
    "\n",
    "Градиент среднеквадратической ошибки:\n",
    "\n",
    "$$ \\nabla MSE = \\frac {1}{n} (\\nabla Y^T Y - 2 \\nabla w^T X^T Y + \\nabla w^T X^T X w) = \\frac {1}{n} (0 - 2 X^T Y + 2 X^T X w) = \\frac {2}{n} (X^T X w - X^T Y) = \\frac {2}{n} X^T (X w - Y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Новое значение параметра весов при градиентном спуске:\n",
    "\n",
    "$$ w_{t+1} = w_t - \\mu \\nabla MSE $$\n",
    "\n",
    "* $ \\mu $ - скорость обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Градиент спуск по сгенерированным точкам**\n",
    "\n",
    "В качестве функций потерь будут взяты тестовые функции для оптимизации, градиент которых будет вычисляться, исходя из положения начальной точки. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Стохастический градиентный спуск**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм, вычисляющий параметр не за счёт использования всего набора данных, а только для его $ i $-х точек:\n",
    "\n",
    "$$ w_{t+1} = w_t - \\mu \\nabla f_i (w_t) = w_t - \\mu \\frac {\\delta}{\\delta w_t} f_i (w_t) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Алгоритм:**\n",
    ">\n",
    "> 1. В аргументах алгоритма задаётся начальное значение параметра.\n",
    "> 2. Обучающая выборка разбивается на случайные $ i $-e точки.\n",
    "> 3. Рассчитывается градиент функции потерь.\n",
    "> 4. Вычисляется новое значение параметра."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Стохастический градиентный спуск по мини-батчам**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм, вычисляющий параметр при разбитых на части перемешанных данных выборки.\n",
    "\n",
    "* Число батчей соответствует числу итераций для завершения одной эпохи.\n",
    "* Число эпох зависит от размера обучающей выборки.\n",
    "* Эпоха завершается, когда алгоритм SGD проходит один раз по всем батчам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Алгоритм:**\n",
    ">\n",
    "> 1. В аргументах алгоритма задаётся начальное значение параметра и размер батча.\n",
    "> 2. Обучающая выборка разбивается на батчи для каждой эпохи.\n",
    "> 3. Рассчитывается градиент функции потерь.\n",
    "> 3. Вычисляется новое значение параметра для каждого батча."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; opacity: 0.2; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Реализация алгоритмов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объявление среднеквадратичной функции потерь для набора данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_grad(X: pd.Series, \n",
    "             Y: pd.Series, \n",
    "             w: np.array) -> np.array:\n",
    "    \n",
    "    \"\"\"Calculate mean squared error gradient\n",
    "        \n",
    "       Args:\n",
    "           X (pd.Series): features matrix with ONES column\n",
    "           Y (pd.Series): target vector\n",
    "           w (np.array): weights\n",
    "\n",
    "       Return:\n",
    "           Tuple (np.array): gradients\n",
    "    \"\"\"\n",
    "    \n",
    "    y_hat = X @ w\n",
    "    error = X.T @ y_hat - X.T @ Y\n",
    "    grad = (2 / X.shape[0]) * error\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объявление среднеквадратичной функции потерь для набора точек:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_grad(X, Y, w, grad_f: Mapping):\n",
    "    X_len = X.shape[0]\n",
    "    \n",
    "    try:\n",
    "        X, Y = grad_f(*X, *Y)\n",
    "        \n",
    "    except:\n",
    "        X, Y = grad_f(X, Y)\n",
    "        X, Y = X.mean(), Y.mean()\n",
    "    \n",
    "    y_hat = X * grad_f(*w)\n",
    "    error = X * y_hat - X * Y\n",
    "    grad = (2 / X_len) * error\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объявление класса градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent():      \n",
    "    results = {'weights': [],\n",
    "               'grads': []}\n",
    "    \n",
    "    \n",
    "    def __init__(self,\n",
    "                 X: [np.array, pd.Series],\n",
    "                 Y: [np.array, pd.Series],\n",
    "                 grad_f: Mapping = None,\n",
    "                 minim_f: Mapping = mse_grad,\n",
    "                 seed: int = 2020,\n",
    "                 epsilon: int = 2e-4):\n",
    "        \n",
    "        \"\"\"Initialize GradientDescent\n",
    "        \n",
    "           Args:\n",
    "               X (np.array, pd.Series): features matrix\n",
    "               Y (np.array, pd.Series): target vector\n",
    "               grad_f (Mapping, optional): gradient function. Defaults to None\n",
    "               minim_f (Mapping, optional): minimization function. Defaults to mse_grad\n",
    "               seed(int, optional): seed value. Defaults to 2020\n",
    "               epsilon (int, optional): epsilon value. Defaults to 2e-4\n",
    "        \"\"\"\n",
    "        \n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.grad_f = grad_f\n",
    "        self.minim_f = minim_f\n",
    "        self.seed = seed\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.X_ones = np.hstack(((np.ones((X.shape[0], 1))), X[:, None]))\n",
    "        \n",
    "        if seed is None:\n",
    "            np.random.seed(0)\n",
    "        \n",
    "    \n",
    "    def descent(self,  \n",
    "                w0: tuple,\n",
    "                descent: str,\n",
    "                learn_r: float = 0.01,\n",
    "                max_iters: int = 300,\n",
    "                batch_size: int = None) -> Tuple [list, list]:\n",
    "        \n",
    "        \"\"\"Gradient descent calculation\n",
    "        \n",
    "           Args:\n",
    "               w0 (tuple): start weights\n",
    "               descent (string, optional): type of SGD\n",
    "               learn_r (float, optional): learning rate. Defaults to 0.01\n",
    "               max_iters (int, optional): number of max iterations. Defaults to 300\n",
    "               batch_size (int, optional): batch size. Defaults to None\n",
    "                \n",
    "           Return:\n",
    "               Tuple [list, list]: weights, grads\n",
    "        \"\"\"\n",
    "        \n",
    "        if descent == 'gradient':\n",
    "            return self.gradient_descent(w0, learn_r, max_iters, descent,)\n",
    "                \n",
    "        elif descent == 'stochastic':\n",
    "            return self.stochastic_descent(w0, learn_r, max_iters, descent)\n",
    "            \n",
    "        elif descent == 'minibatch':\n",
    "            return self.minibatch_descent(w0, learn_r, max_iters, batch_size, descent)\n",
    "        \n",
    "        else:\n",
    "            raise Exception('Incorrect type of descent')\n",
    "    \n",
    "    \n",
    "    def gradient_descent(self, w0, learn_r, max_iters, descent) -> dict:\n",
    "        \"\"\"Gradient descent algorithm\"\"\"\n",
    "        \n",
    "        self.clear_results(w0)\n",
    "        \n",
    "        w_old = w0\n",
    "        iters = 1\n",
    "        dw = np.array(2 * self.epsilon)\n",
    "        \n",
    "        while abs(dw.sum()) > self.epsilon and iters <= max_iters:\n",
    "            w_new, dw = self.get_gradients(self.X_ones, self.X, self.Y, \n",
    "                                           w_old, learn_r, descent)\n",
    "            w_old = w_new\n",
    "            iters += 1\n",
    "            \n",
    "        self.print_min(w_old)\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    \n",
    "    def stochastic_descent(self, w0, learn_r, max_iters, descent) -> dict:\n",
    "        \"\"\"Stochastic gradient descent algorithm\"\"\"\n",
    "        \n",
    "        self.clear_results(w0)\n",
    "        \n",
    "        w_old = w0\n",
    "        \n",
    "        for _ in range(max_iters):\n",
    "            i = np.random.randint(self.X_ones.shape[0])\n",
    "            w_new = self.get_gradients(self.X_ones[i, None], self.X[i, None], \n",
    "                                       self.Y[i, None], w_old, learn_r, descent)[0]\n",
    "            \n",
    "            w_old = w_new\n",
    "            \n",
    "        self.print_min(w_old)\n",
    "            \n",
    "        return self.results\n",
    "    \n",
    "    \n",
    "    def minibatch_descent(self, w0, learn_r, max_iters, batch_size, descent) -> dict:\n",
    "        \"\"\"Stochastic mini-batch gradient descent algorithm\"\"\"\n",
    "        \n",
    "        self.clear_results(w0)\n",
    "    \n",
    "        w_old = w0\n",
    "        \n",
    "        for _ in range(max_iters):\n",
    "            batches_count = self.X.shape[0] // batch_size\n",
    "            \n",
    "            for i in range(batches_count):\n",
    "                begin = i * batch_size\n",
    "                end = (i + 1) * batch_size\n",
    "                \n",
    "                w_new = self.get_gradients(self.X_ones[begin:end, :], self.X[begin:end], \n",
    "                                           self.Y[begin:end], w_old, learn_r, descent)[0]\n",
    "                    \n",
    "            w_old = w_new\n",
    "            \n",
    "        self.print_min(w_old)\n",
    "            \n",
    "        return self.results\n",
    "    \n",
    "    \n",
    "    def get_gradients(self, X_ones, X, Y, w_old, learn_r, descent):\n",
    "        \"\"\"Calculate gradients per each loop saving the result\"\"\"\n",
    "        \n",
    "        try:\n",
    "            grad = self.minim_f(X_ones, Y, w_old)\n",
    "            \n",
    "        except:\n",
    "            if descent == 'gradient':\n",
    "                grad = self.grad_f(*w_old)\n",
    "                \n",
    "            else:\n",
    "                grad = self.minim_f(X, Y, w_old, self.grad_f)\n",
    "        \n",
    "        dw = learn_r * grad\n",
    "        \n",
    "        if descent == 'minibatch' and len(self.results['weights']) > 2:\n",
    "            dw *= 0.6\n",
    "        \n",
    "        w_new = w_old - dw\n",
    "\n",
    "        self.results['weights'].append(list(w_new))\n",
    "        self.results['grads'].append(list(grad))\n",
    "        \n",
    "        return w_new, dw\n",
    "    \n",
    "    \n",
    "    def clear_results(self, w0):\n",
    "        \"\"\"Clear results\"\"\"\n",
    "        self.results['weights'] = [w0]\n",
    "        self.results['grads'] = [[0, 0]]\n",
    "    \n",
    "    \n",
    "    def print_min(self, w):\n",
    "        \"\"\"Print terminated minimum\"\"\"\n",
    "        \n",
    "        print('Got to the min:')\n",
    "        print(f'w0: {w[0]:.2f}\\nw1: {w[1]:.2f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объявление экземпляра класса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd = GradientDescent(x_pokemon, y_pokemon, mse_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; opacity: 0.2; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1. Градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение в переменную результатов вычисления градиента:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd.grads = gd.descent([-20, -10], 'gradient', 0.01, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика линий уровня с градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_loss_2d(x_pokemon, y_pokemon, \n",
    "                              weights=gd.grads['weights'], \n",
    "                              title='Gradient Descent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2. Стохастический градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение в переменную результатов вычисления градиента:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd.grads_stochastic = gd.descent([-20, -10], 'stochastic', 0.01, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика линий уровня с градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_loss_2d(x_pokemon, y_pokemon, \n",
    "                              weights=gd.grads_stochastic['weights'],\n",
    "                              title='Stochastic Gradient Descent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2. Стохастический градиентный спуск по мини-батчам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение в переменную результатов вычисления градиента:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd.grads_minibatch = gd.descent([-20, -10], 'minibatch', 0.001, 3500, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика линий уровня с градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_loss_2d(x_pokemon, y_pokemon, \n",
    "                              weights=gd.grads_minibatch['weights'], \n",
    "                              title='Mini-Batch Gradient Descent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Вывод**\n",
    ">\n",
    ">Реализованы алгоритмы вычисления градиентного спуска."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Алгоритмы оптимизации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Математическое обоснование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Оптимизатор Adam**\n",
    "\n",
    "Адаптивный метод оптимизации (достижение лучших результатов, ускоряющих обучение). Сочетает в себе и идею накопления движения и идею более слабого обновления весов для типичных признаков.\n",
    "\n",
    "$$ w_t = w_{t-1} - \\frac {\\mu}{\\sqrt {\\hat v_t + \\epsilon}} \\hat m_t $$\n",
    "\n",
    "* $ \\hat m_t $ - первый момент\n",
    "* $ \\hat v_t $ - второй момент\n",
    "\n",
    "Искусственное увеличение $ m_t, v_t $ на первых шагах:\n",
    "\n",
    "$$ m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t $$ \n",
    "$$ \\hat m_t = \\frac {m_t}{1 - \\beta_1^t} $$\n",
    "\n",
    "$$ v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 $$ \n",
    "$$ \\hat v_t = \\frac {v_t}{1 - \\beta_2^t} $$\n",
    "\n",
    "* $ \\beta_1, \\beta_2 $ - дополнительные параметры (0.9 и 0.999 соответственно)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Метод моментов (импульсов)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод оптимизации, придающий вес инерции (момента) к вычисляемым градиентам. Прежнее значение градиента умножается на некоторый множитель и к нему прибавляется текущее значение градиента:\n",
    "\n",
    "$$ \\Delta W_t = - \\eta \\frac {\\delta Error}{\\delta W_t} + \\alpha \\Delta W_{t-1} $$\n",
    "\n",
    "\n",
    "* $ \\alpha $ - коэффициент момента\n",
    "\n",
    "> При $ \\alpha = 0 $ результатом будет исходный стохастический градиентный спуск. При значении $ \\alpha = 1 $ оптимизатор учитываюет всю историю произошедших изменений параметров.\n",
    "\n",
    "\n",
    "$$ W_t += \\Delta W_t $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; opacity: 0.2; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Реализация алгоритмов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объявление класса алгоритмов оптимизации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimization:\n",
    "    results = []\n",
    "\n",
    "    def __init__(self, \n",
    "                 weights: list, \n",
    "                 grads: list, \n",
    "                 beta1: float = 0.9, \n",
    "                 beta2: float = 0.999,\n",
    "                 epsilon: int = 2e-4):\n",
    "        \n",
    "        \"\"\"Initialize Optimization\n",
    "        \n",
    "        Args:\n",
    "               weights (list): gradient weights\n",
    "               grads (list): gradients\n",
    "               beta1 (float, optional): 1st coefficient. Defaults to 0.9\n",
    "               beta2 (float, optional): 2nd coefficient. Defaults to 0.999\n",
    "               epsilon (int, optional): epsilon value. Defaults to 2e-4\n",
    "        \"\"\"\n",
    "        \n",
    "        self.weights = np.array(weights)\n",
    "        self.grads = np.array(grads)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    \n",
    "    def optimize(self, \n",
    "                 optimizer: str,\n",
    "                 learn_r: float = 0.01) -> np.array:\n",
    "        \n",
    "        \"\"\"Calculate optimization\n",
    "           \n",
    "           Args:\n",
    "               optimizer (str): type of optimizer\n",
    "               learn_r (float, optional): learning rate. Defaults to 0.01\n",
    "           \n",
    "           Return:\n",
    "               np.array: updated weights\n",
    "               \n",
    "        \"\"\"\n",
    "            \n",
    "        if optimizer == 'adam':\n",
    "            return self.adam_optimizer(learn_r)\n",
    "            \n",
    "        elif optimizer == 'momentum':\n",
    "            return self.momentum_optimizer(learn_r)\n",
    "            \n",
    "        else:\n",
    "            raise Exception('Incorrect type of optimization')\n",
    "    \n",
    "    \n",
    "    def adam_optimizer(self, learn_r) -> np.array:\n",
    "        \"\"\"Adam optimization\"\"\"\n",
    "        \n",
    "        self.clear_results()\n",
    "        \n",
    "        m = [0] * self.weights.shape[0]\n",
    "        v = [0] * self.weights.shape[0]\n",
    "        \n",
    "        iters = 1\n",
    "        \n",
    "        for i, (weight, grad) in enumerate(zip(self.weights, self.grads)): \n",
    "            m[i] = self.beta1 * m[i] + (1 - self.beta1) * grad          \n",
    "            v[i] = self.beta2 * v[i] + (1 - self.beta2) * grad ** 2\n",
    "\n",
    "            m_corrected = m[i] / (1 - self.beta1 ** iters)\n",
    "            v_corrected = v[i] / (1 - self.beta2 ** iters)\n",
    "\n",
    "            new_weight = weight - learn_r * m_corrected / (np.sqrt(v_corrected) + self.epsilon)\n",
    "        \n",
    "            self.results.append(new_weight)\n",
    "            iters += 1\n",
    "            \n",
    "        self.print_min(self.results[-1])\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "\n",
    "    def momentum_optimizer(self, learn_r) -> np.array:\n",
    "        \"\"\"Momentum optimization\"\"\"\n",
    "        \n",
    "        self.clear_results()\n",
    "        \n",
    "        previous_updates = [0] * self.weights.shape[0]\n",
    "        prevs = []\n",
    "        mom_coeff = 1\n",
    "        \n",
    "        for weight, grad, prev_update in zip(self.weights, self.grads, previous_updates):\n",
    "            delta = learn_r * grad - mom_coeff * prev_update\n",
    "            new_weight = weight - delta\n",
    "            \n",
    "            prevs.append(delta)\n",
    "            self.results.append(new_weight)\n",
    "            previous_updates = prevs  \n",
    "        \n",
    "        self.print_min(self.results[-1])\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    \n",
    "    def clear_results(self):\n",
    "        self.results = []\n",
    "    \n",
    "    \n",
    "    def print_min(self, w):\n",
    "        \"\"\"Print terminated minimum\"\"\"\n",
    "        \n",
    "        print('Got to the min:')\n",
    "        print(f'w0: {w[0]:.2f}\\nw1: {w[1]:.2f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; opacity: 0.2; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объявление экземпляра класса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd.optimizer = Optimization(*gd.grads_stochastic.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1. ADAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление оптимизированного градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd.optim_adam = gd.optimizer.optimize('adam', 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика функции с оптимизированным градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_loss_2d(x_pokemon, y_pokemon, \n",
    "                              weights=gd.optim_adam, \n",
    "                              title='ADAM SGD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2. Метод моментов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление оптимизированного градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd.optim_momentum = gd.optimizer.optimize('momentum', 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика функции с оптимизированным градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_loss_2d(x_pokemon, y_pokemon, \n",
    "                              weights=gd.optim_momentum, \n",
    "                              title='Momentum SGD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Вывод**\n",
    ">\n",
    "> Реализованы алгоритмы адаптивной и моментной оптимизаций."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Искусственные ландшафты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для установления качества работы алгоритмов будут рассмотрены следующие ландшафты, образованные сложными функциями: \n",
    "\n",
    "* Функция МакКормика\n",
    "* Функция Изома\n",
    "* Функция Растригина"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; opacity: 0.2; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Функция МакКормика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция МакКормика выглядит следующим образом:\n",
    "\n",
    "$$ f(x, y) = \\sin (x + y) + (x - y)^2 - 1.5x + 2.5y + 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание функции потерь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mccormick_loss = lambda x, y: np.sin(x + y) + (x - y) ** 2 - 1.5 * x + 2.5 * y + 1\n",
    "mccormick_deriv = sin(x + y) + (x - y) ** 2 - 1.5 * x + 2.5 * y + 1\n",
    "\n",
    "mccormick_min = [-4, -4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графиков функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_function_gradient_2d_3d(x_linear, y_linear, mccormick_loss, 'McCornick')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран частных производных каждой переменной:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mccormick_deriv.diff(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mccormick_deriv.diff(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание функций вычисления частных производных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mccormick_partials(X, Y):\n",
    "    partial_x = lambda x, y: 2 * x - 2 * y + np.cos(x + y) - 1.5\n",
    "    partial_y = lambda x, y: -2 * x + 2 * y + np.cos(x + y) + 2.5\n",
    "    \n",
    "    partials = np.array([partial_x(X, Y), partial_y(X, Y)])\n",
    "    \n",
    "    return partials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; opacity: 0.2; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определение экземпляра класса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_mccormick = GradientDescent(x_linear, y_linear, mccormick_partials, func_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Градиентный спуск**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_mccormick.grads = gd_mccormick.descent([-2, 3], 'gradient', 0.01, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика линий уровня с градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_function_2d(x_linear, y_linear,\n",
    "                                  gd_mccormick.grads['weights'],\n",
    "                                  mccormick_loss,\n",
    "                                  'SGD Depth McCormick function',\n",
    "                                  mccormick_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Стохастический градиентный спуск**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_mccormick.grads_stochastic = gd_mccormick.descent([-2, 3], 'stochastic', 0.0001, 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика линий уровня с градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_function_2d(x_linear, y_linear,\n",
    "                                  gd_mccormick.grads_stochastic['weights'],\n",
    "                                  mccormick_loss,\n",
    "                                  'SGD Depth McCormick function',\n",
    "                                  mccormick_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика функции потерь с градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_3d(x_linear, y_linear,\n",
    "                         mccormick_loss,\n",
    "                         'SGD 3D McCormick function',\n",
    "                         gd_mccormick.grads_stochastic['weights'],\n",
    "                         descent=True,\n",
    "                         global_min=mccormick_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Стохастический градиентный спуск по мини-батчам**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_mccormick.grads_minibatch = gd_mccormick.descent([-2, 3], 'minibatch', 0.01, 800, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика линий уровня с градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_function_2d(x_linear, y_linear,\n",
    "                                  gd_mccormick.grads_minibatch['weights'],\n",
    "                                  mccormick_loss,\n",
    "                                  'SGD Depth McCormick function',\n",
    "                                  mccormick_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; opacity: 0.2; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для оптимизации будут взяты значения стохастического градиентного спуска по мини-батчам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объявление экземпляра класса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_mccormick.optimizer = Optimization(*gd_mccormick.grads_minibatch.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление оптимизированного градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_mccormick.optim_adam = gd_mccormick.optimizer.optimize('adam', 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика функции с оптимизированным градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_function_2d(x_linear, y_linear,\n",
    "                                  gd_mccormick.optim_adam,\n",
    "                                  mccormick_loss,\n",
    "                                  'ADAM Optimizer Depth McCornick function',\n",
    "                                  mccormick_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Метод моментов**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление оптимизированного градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_mccormick.optim_moment = gd_mccormick.optimizer.optimize('momentum', 0.08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика функции с оптимизированным градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_function_2d(x_linear, y_linear,\n",
    "                                  gd_mccormick.optim_moment,\n",
    "                                  mccormick_loss,\n",
    "                                  'Momentum Optimizer Depth McCornick function',\n",
    "                                  mccormick_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; opacity: 0.2; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Функция Изома"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция Изома выглядит следующим образом:\n",
    "\n",
    "$$ f(x, y) = -cos(x) cos(x) exp \\Bigg(- \\bigg( (x - \\pi)^2 + (y - \\pi)^2 \\bigg) \\Bigg) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание функции потерь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "izom_loss = lambda x, y: -np.cos(x) * np.cos(x) * np.exp(-((x - np.pi) ** 2 + (y - np.pi) ** 2))\n",
    "izom_deriv = -cos(x) * cos(x) * exp(-((x - pi) ** 2 + (y - pi) ** 2))\n",
    "\n",
    "izom_min = [3.14, 3.14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графиков функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_function_gradient_2d_3d(x_linear, y_linear, izom_loss, 'Izom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран частных производных каждой переменной:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "izom_deriv.diff(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "izom_deriv.diff(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание функций вычисления частных производных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def izom_partials(X, Y):\n",
    "    partial_x = lambda x, y: -(-2 * x + 2 * np.pi) * np.exp((-(x - np.pi) ** 2 - (y - np.pi) ** 2)) * \\\n",
    "                             np.cos(x) ** 2 + 2 * np.exp(-(x - np.pi) ** 2 - (y - np.pi) ** 2) * \\\n",
    "                             np.sin(x) * np.cos(x)\n",
    "    partial_y = lambda x, y: -(-2 * y + 2 * np.pi) * np.exp(-(x - np.pi) ** 2 - (y - np.pi) ** 2) * np.cos(x) ** 2\n",
    "    \n",
    "    partials = np.array([partial_x(X, Y), partial_y(X, Y)])\n",
    "    \n",
    "    return partials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; opacity: 0.2; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определение экземпляра класса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_izom = GradientDescent(x_linear, y_linear, izom_partials, func_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Градиентный спуск**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_izom.grads = gd_izom.descent([-2, -3], 'gradient', 0.1, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика линий уровня с градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_function_2d(x_linear, y_linear,\n",
    "                                  gd_izom.grads['weights'],\n",
    "                                  izom_loss,\n",
    "                                  'SGD Depth McCormick function',\n",
    "                                  izom_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Стохастический градиентный спуск**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_izom.grads_stochastic = gd_izom.descent([-2, -3], 'stochastic', 0.001, 55000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика линий уровня с градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_function_2d(x_linear, y_linear,\n",
    "                                  gd_izom.grads_stochastic['weights'],\n",
    "                                  izom_loss,\n",
    "                                  'SGD 2D Izom descent',\n",
    "                                  izom_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика функции потерь с градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_3d(x_linear, y_linear,\n",
    "                         izom_loss,\n",
    "                         'SGD 3D Izom function',\n",
    "                         gd_izom.grads_stochastic['weights'],\n",
    "                         descent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Стохастический градиентный спуск по мини-батчам**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_izom.grads_minibatch = gd_izom.descent([-2, -3], 'stochastic', 0.001, 55000, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика линий уровня с градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_function_2d(x_linear, y_linear,\n",
    "                                  gd_izom.grads_minibatch['weights'],\n",
    "                                  izom_loss,\n",
    "                                  'SGD 2D Izom descent',\n",
    "                                  izom_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; opacity: 0.2; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для оптимизации будут взяты значения стохастического градиентного спуска по мини-батчам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объявление экземпляра класса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_izom.optimizer = Optimization(*gd_izom.grads_minibatch.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление оптимизированного градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_izom.optim_adam = gd_izom.optimizer.optimize('adam', 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика функции с оптимизированным градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_function_2d(x_linear, y_linear,\n",
    "                                  gd_izom.optim_adam,\n",
    "                                  izom_loss,\n",
    "                                  'ADAM Optimizer Depth Izom function',\n",
    "                                  izom_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Метод моментов**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление оптимизированного градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_izom.optim_moment = gd_izom.optimizer.optimize('momentum', 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика функции с оптимизированным градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_function_2d(x_linear, y_linear,\n",
    "                                  gd_izom.optim_moment,\n",
    "                                  izom_loss,\n",
    "                                  'Momentum Optimizer Depth Izom function',\n",
    "                                  izom_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; opacity: 0.2; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Функция Растригина"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция Растригина выглядит следующим образом:\n",
    "\n",
    "$$ f(x) = An + \\sum_{i=1}^n \\big[x_i^2 - A cos(2 \\pi x_i) \\big], A =10 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание функции потерь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rastrigin_loss(*X, A: int = 10):\n",
    "    return A + sum([(x ** 2 - A * np.cos(2 * np.pi * x)) for x in X])\n",
    "\n",
    "rastrigin_deriv = 10 + (x**2 - 10 * cos(2 * pi * x))\n",
    "\n",
    "rastrigin_min = [0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графиков функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_function_gradient_2d_3d(x_linear, y_linear, rastrigin_loss, 'Rastrigin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран частных производных каждой переменной:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff(rastrigin_deriv, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание функций вычисления частных производных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rastrigin_partials(X, Y):\n",
    "    partial_x = lambda x: 2 * x + 20 * np.pi * np.sin(2 * np.pi * x)\n",
    "    \n",
    "    partials = np.array([partial_x(X), Y])\n",
    "    \n",
    "    return partials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определение экземпляра класса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_rastrigin = GradientDescent(x_linear, y_linear, rastrigin_partials, func_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; opacity: 0.2; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Градиентный спуск**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_rastrigin.grads = gd_rastrigin.descent([-3, -3], 'gradient', 0.01, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика линий уровня с градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_function_2d(x_linear, y_linear,\n",
    "                                  gd_rastrigin.grads['weights'],\n",
    "                                  rastrigin_loss,\n",
    "                                  'SGD Depth Rastrigin function',\n",
    "                                  rastrigin_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Стохастический градиентный спуск**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_rastrigin.grads_stochastic = gd_rastrigin.descent([-3, -3], 'stochastic', 0.0001, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика линий уровня с градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_function_2d(x_linear, y_linear,\n",
    "                                  gd_rastrigin.grads_stochastic['weights'],\n",
    "                                  rastrigin_loss,\n",
    "                                  'SGD Depth Rastrigin function',\n",
    "                                  rastrigin_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика функции потерь с градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_3d(x_linear, y_linear,\n",
    "                         rastrigin_loss,\n",
    "                         'SGD 3D Rastrigin function',\n",
    "                         gd_rastrigin.grads_stochastic['weights'],\n",
    "                         descent=True,\n",
    "                         global_min=rastrigin_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Стохастический градиентный спуск по мини-батчам**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_rastrigin.grads_minibatch = gd_rastrigin.descent([-1.5, -3.5], 'minibatch', 0.01, 100, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика линий уровня с градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_function_2d(x_linear, y_linear,\n",
    "                                  gd_rastrigin.grads_minibatch['weights'],\n",
    "                                  rastrigin_loss,\n",
    "                                  'SGD Depth Rastrigin function',\n",
    "                                  rastrigin_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; opacity: 0.2; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объявление экземпляра класса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_rastrigin.optimizer = Optimization(*gd_rastrigin.grads_stochastic.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление оптимизированного градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_rastrigin.optim_adam = gd_rastrigin.optimizer.optimize('adam', 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика функции с оптимизированным градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_function_2d(x_linear, y_linear,\n",
    "                                  gd_rastrigin.optim_adam,\n",
    "                                  rastrigin_loss,\n",
    "                                  'ADAM Optimizer Depth Rastrigin function',\n",
    "                                  [0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Метод моментов**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление оптимизированного градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_rastrigin.optim_moment = gd_rastrigin.optimizer.optimize('momentum', 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика функции с оптимизированным градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_function_2d(x_linear, y_linear,\n",
    "                                  gd_rastrigin.optim_moment,\n",
    "                                  rastrigin_loss,\n",
    "                                  'Momentum Optimizer Depth Izom function',\n",
    "                                  [0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Вывод**\n",
    ">\n",
    "> Проведено тестирование алгоритмов градиентного спуска, а также адаптивной и моментной оптимизаций на тестовых функциях для оптимизации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; opacity: 0.5; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Общий вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполнена реализация алгоритмов:\n",
    "\n",
    "* Градиентного спуска и стохастического градиентного спуска.\n",
    "\n",
    "* Моментной и адаптивной оптимизаций.\n",
    "\n",
    "Реализация включает в себя демонстрацию работы алгоритмов как на наборе данных, так и на сгенерированных точках.\n",
    "\n",
    "Также проведено тестирование алгоритма стохастического градиентного спуска на разных тестовых функциях для оптимизации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size: 20px; padding: 15px 0;\">\n",
    "    <a href=\"#Содержание\" data-toc-modified-id=\"Содержание\" style=\"text-decoration: none; color: #296eaa; border: 2px dashed #296eaa; opacity: 0.8; border-radius: 3px; padding: 10px 80px;\">\n",
    "        Наверх к содержанию ↑\n",
    "    </a>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
