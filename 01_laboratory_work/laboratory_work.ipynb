{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<br>\n",
    "<div class=\"toc\">\n",
    "    <ul class=\"toc-item\">\n",
    "        <li>\n",
    "            <span>\n",
    "                <a href=\"#1.-Импорт-библиотек\">\n",
    "                    <span class=\"toc-item-num\">1.&nbsp;&nbsp;</span>\n",
    "                    Импорт библиотек\n",
    "                </a>\n",
    "            </span>\n",
    "        </li>\n",
    "        <li>\n",
    "            <span>\n",
    "                <a href=\"#2.-Извлечение-данных-из-файла\">\n",
    "                    <span class=\"toc-item-num\">2.&nbsp;&nbsp;</span>\n",
    "                    Извлечение данных из файла\n",
    "                </a>\n",
    "            </span>\n",
    "        </li>\n",
    "        <li style=\"margin-top: 10px\">\n",
    "            <span>\n",
    "                <a href=\"#3.-Алгоритмы-градиентного-спуска\">\n",
    "                    <span class=\"toc-item-num\">3.&nbsp;&nbsp;</span>\n",
    "                    Алгоритмы градиентного спуска\n",
    "                </a>\n",
    "            </span>\n",
    "            <ul class=\"toc-item\">\n",
    "                <li>\n",
    "                    <span>\n",
    "                        <a href=\"#3.1.-Градиентный-спуск\">\n",
    "                            <span class=\"toc-item-num\">3.1.&nbsp;&nbsp;</span>\n",
    "                            Градиентный спуск\n",
    "                        </a>\n",
    "                    </span>\n",
    "                    <ul class=\"toc-item\">\n",
    "                        <li>\n",
    "                            <span>\n",
    "                                <a href=\"#3.1.1.-Математическое-обоснование\">\n",
    "                                    <span class=\"toc-item-num\">3.1.1.&nbsp;&nbsp;</span>\n",
    "                                    Математическое обоснование\n",
    "                                </a>\n",
    "                            </span>\n",
    "                        </li>\n",
    "                        <li>\n",
    "                            <span>\n",
    "                                <a href=\"#3.1.2.-Реализация-алгоритма\">\n",
    "                                    <span class=\"toc-item-num\">3.1.2.&nbsp;&nbsp;</span>\n",
    "                                    Реализация алгоритма\n",
    "                                </a>\n",
    "                            </span>\n",
    "                        </li>\n",
    "                    </ul>\n",
    "                </li>\n",
    "                <li>\n",
    "                    <span>\n",
    "                        <a href=\"#3.2.-Стохастический-градиентный-спуск\">\n",
    "                            <span class=\"toc-item-num\">3.2.&nbsp;&nbsp;</span>\n",
    "                            Стохастический градиентный спуск\n",
    "                        </a>\n",
    "                    </span>\n",
    "                    <ul class=\"toc-item\">\n",
    "                        <li>\n",
    "                            <span>\n",
    "                                <a href=\"#3.2.1.-Математическое-обоснование\">\n",
    "                                    <span class=\"toc-item-num\">3.2.1.&nbsp;&nbsp;</span>\n",
    "                                    Математическое обоснование\n",
    "                                </a>\n",
    "                            </span>\n",
    "                        </li>\n",
    "                        <li>\n",
    "                            <span>\n",
    "                                <a href=\"#3.2.2.-Реализация-алгоритма\">\n",
    "                                    <span class=\"toc-item-num\">3.2.2.&nbsp;&nbsp;</span>\n",
    "                                    Реализация алгоритма\n",
    "                                </a>\n",
    "                            </span>\n",
    "                        </li>\n",
    "                    </ul>\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li style=\"margin-top: 10px\">\n",
    "            <span>\n",
    "                <a href=\"#4.-Алгоритмы-оптимизации\">\n",
    "                    <span class=\"toc-item-num\">4.&nbsp;&nbsp;</span>\n",
    "                    Алгоритмы оптимизации\n",
    "                </a>\n",
    "            </span>\n",
    "            <ul>\n",
    "                <li>\n",
    "                    <span>\n",
    "                        <a href=\"#4.1.-Математическое-обоснование\">\n",
    "                            <span class=\"toc-item-num\">4.1.&nbsp;&nbsp;</span>\n",
    "                            Математическое обоснование\n",
    "                        </a>\n",
    "                    </span>\n",
    "                </li>\n",
    "                <li>\n",
    "                    <span>\n",
    "                        <a href=\"#4.2.-Реализация-алгоритма\">\n",
    "                            <span class=\"toc-item-num\">4.2.&nbsp;&nbsp;</span>\n",
    "                            Реализация алгоритма\n",
    "                        </a>\n",
    "                    </span>\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li style=\"margin-top: 10px\">\n",
    "            <span>\n",
    "                <a href=\"#5.-Искусственные-ландшафты\">\n",
    "                    <span class=\"toc-item-num\">5.&nbsp;&nbsp;</span>\n",
    "                    Искусственные ландшафты\n",
    "                </a>\n",
    "            </span>\n",
    "            <ul>\n",
    "                <li>\n",
    "                    <span>\n",
    "                        <a href=\"#5.1.-Функция-Матьяса\">\n",
    "                            <span class=\"toc-item-num\">5.1.&nbsp;&nbsp;</span>\n",
    "                            Функция Матьяса\n",
    "                        </a>\n",
    "                    </span>\n",
    "                </li>\n",
    "                <li>\n",
    "                    <span>\n",
    "                        <a href=\"#5.2.-Функция-Изома\">\n",
    "                            <span class=\"toc-item-num\">5.2.&nbsp;&nbsp;</span>\n",
    "                            Функция Изома\n",
    "                        </a>\n",
    "                    </span>\n",
    "                </li>\n",
    "                <li>\n",
    "                    <span>\n",
    "                        <a href=\"#5.3.-Функция-Химмельблау\">\n",
    "                            <span class=\"toc-item-num\">5.1.&nbsp;&nbsp;</span>\n",
    "                            Функция Химмельблау\n",
    "                        </a>\n",
    "                    </span>\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li style=\"margin-top: 10px\">\n",
    "            <span>\n",
    "                <a href=\"#6.-Общий-вывод\">\n",
    "                    <span class=\"toc-item-num\">6.&nbsp;&nbsp;</span>\n",
    "                    Общий вывод\n",
    "                </a>\n",
    "            </span>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа №1: Gradient Descend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача:** запрограммировать собственную реализацию Стохастического градиентного спуска для 1) моментного и 2) адаптивного методов.\n",
    "\n",
    "**Тестирование:** протестировать работу алгоритма стохастического градиентного спуска на искусственных ландшафтах, взятых со страницы: [Тестовые функции для оптимизации](https://ru.wikipedia.org/wiki/Тестовые_функции_для_оптимизации).\n",
    "\n",
    "**Источник данных:** набор данных взят с платформы [Kaggle](https://www.kaggle.com/datasets/rounakbanik/pokemon).\n",
    "\n",
    "**Описание данных:** набор данных содержит информацию обо всех семи поколениях 802 Покемонов.\n",
    "\n",
    "---\n",
    "\n",
    "Для реализации поставленных задач из набора данных `datasets/pokemon.csv` будут взяты следующие столбцы:\n",
    "\n",
    "* `defense` - признак объектов\n",
    "* `attack` - целевой признак\n",
    "* `name` - имя Покемона"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Импорт библиотек"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Установка необходимых библиотек:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sympy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импорт всех необходимых библиотек:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Tuple, Mapping\n",
    "\n",
    "from sympy.abc import x, y\n",
    "from sympy import diff\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "from utils.plot_charts import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Внутрипроектный модуль `plot_charts` включает в себя следующие функции:\n",
    "\n",
    "* `plot_2d_chart` - построение двумерного графика\n",
    "\n",
    "* `plot_animated_2d_chart` - построение анимированного двумерного графика с демонстрацией градиентного спуска\n",
    "\n",
    "* `plot_2d_contours_chart` - построение плоского двумерного графика глубины\n",
    "\n",
    "* `plot_3d_chart` - построение трёхмерного графика\n",
    "\n",
    "* `plot_3d_gradient_chart` - построение трёхмерного графика с градиентным спуском\n",
    "\n",
    "* `plot_animated_3d_chart` - построение анимированного трёхмерного графика с демонстрацией градиентного спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Изучение данных из файла "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение набора данных, состоящего из столбцов `name`, `defense` и `attack`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (pd.read_csv('datasets/pokemon.csv', usecols=['name', 'defense', 'attack'], index_col=0)\n",
    "          .reset_index())[['name', 'defense', 'attack']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран полученного набора данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение в переменные признака объектов и целевого признака:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = StandardScaler().fit_transform(data[['defense']]).flatten()\n",
    "target = data['attack'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран распределения признаков объектов по целевым признакам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(features, target, {'title': 'Распределение признаков объектов по целевым признакам', \n",
    "                             'x': 'features', \n",
    "                             'y': 'target'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Алгоритмы градиентного спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1. Математическое обоснование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Градиентный спуск** - это итеративный алгоритм поиска минимума функции потерь. Иначе говоря, это способ нахождения локального минимума функции потерь в процессе движения в направлении антиградиента.\n",
    "\n",
    "> **Градиент** $ \\nabla f(x) $ показывает направление самого быстого роста функции.\n",
    ">\n",
    "> **Антиградиент** $ - \\nabla f(x) $ показывает направление наискорейшего убывания функции.\n",
    "\n",
    "\n",
    "**Градиент векторной функции** - вектор, состоящий из производных ответа по каждому аргументу.\n",
    "\n",
    "$$ \\nabla f(x) = \\Big( \\frac{\\delta f}{\\delta x_1}, \\frac{\\delta f}{\\delta x_2} ... \\frac{\\delta f}{\\delta x_n} \\Big) $$\n",
    "\n",
    "> **Функция потерь** $L(y,a)$ возвращает число потерь от неправильных ответов модели.\n",
    "\n",
    "$$ L(y,a) = \\sum_{i=1}^n (a_i - y_i)^2 $$\n",
    "\n",
    "* $ y $ - правильные ответы\n",
    "* $ a $ - предсказания\n",
    "\n",
    "Обучение через **минимизацию функции потерь** - это набор таких весов линейной регрессии, чтобы предсказания были максимально близки к правильным ответам.\n",
    "\n",
    "$$ w = argmin_w L(y, a) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Распространённой функцией потерь линейной регрессии является **среднеквадратическая ошибка**:\n",
    "\n",
    "$$ f(y, a) = \\frac {1}{n} \\sum_{i=1}^n (a_i - y_i)^2 $$\n",
    "\n",
    "Где предсказания вычисляются посредством **линейной регресии**, демонстрирующей зависимость переменной $ x $ от одной или нескольких других переменных с линейной функцией зависимости:\n",
    "\n",
    "$$ a = w^T x_i $$\n",
    "$$ a = w_0 + w x_i $$\n",
    "\n",
    "* $ w $ - параметры, минимизирующие MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Алгоритм градиентного спуска**:\n",
    "\n",
    "1. В аргументах алгоритма задаётся начальное значение параметра.\n",
    "2. Рассчитывается градиент функции потерь.\n",
    "3. Вычисление нового значения параметра.\n",
    "\n",
    "Алгоритм градиентного спуска завершается, если выполнено хотя бы одно из условий:\n",
    "\n",
    "* Алгоритм прошёл заданное количество итераций.\n",
    "* Размер шага становится меньше заданного порога."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция потерь:\n",
    "\n",
    "$$ f(w) = \\frac {1}{n} \\sum_{i=1}^{n} (w^T x_i - y_i)^2 $$\n",
    "\n",
    "Градиент функции потерь:\n",
    "\n",
    "$$ \\frac {\\delta}{\\delta w} f(w) = \\frac {2}{n} \\sum_{i=1}^{n} (w^T x_i - y_i)^2 x_i $$\n",
    "\n",
    "Новое значение параметра:\n",
    "\n",
    "$$ w_{t+1} = w_t - \\mu \\nabla f(w_t) = w_t - \\mu \\frac {\\delta}{\\delta w} f(w) $$\n",
    "\n",
    "* $ \\mu $ - размер шага обучения (как правило от 0.001 до 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; opacity: 0.2; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2. Реализация алгоритма"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объявление среднеквадратичной функции потерь с одним параметром:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_gradient(X: [np.array, pd.Series], \n",
    "                 y: [np.array, pd.Series], \n",
    "                 w: list) -> Tuple [np.array, list]:\n",
    "    \n",
    "    \"\"\" Get gradient and mse\n",
    "        \n",
    "        Args:\n",
    "            x (np.array, pd.Series): features with ONES column\n",
    "            y (np.array, pd.Series): target\n",
    "            w (list): weights\n",
    "\n",
    "        Returns:\n",
    "            Tuple [np.array, list]: gradients, mse\n",
    "    \"\"\"\n",
    "    \n",
    "    y_hat = X @ w\n",
    "    error = X.T @ y_hat - X.T @ y\n",
    "    grad = (2 / len(X)) * error\n",
    "    mse = (error ** 2).mean()\n",
    "\n",
    "    return grad, mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объявление класса градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent:    \n",
    "    \n",
    "    def gradient_descent(self,\n",
    "                         x: [np.array, pd.Series],\n",
    "                         y: [np.array, pd.Series], \n",
    "                         w: list, \n",
    "                         grad_f: Mapping,\n",
    "                         learn_r: float = 0.01, \n",
    "                         epsilon: float = 2e-4, \n",
    "                         max_iters: int = 1000) -> Tuple [list, list, list]:\n",
    "        \n",
    "        \"\"\" Regular gradient descent\n",
    "        \n",
    "            Args:\n",
    "                x (np.array, pd.Series): features\n",
    "                y (np.array, pd.Series): target\n",
    "                w (list): weights\n",
    "                grad_f (Mapping): gradient descent function\n",
    "                learn_r (float, optional): learning rate. Defaults to 0.1\n",
    "                epsilon (float, optional): epsilon value. Defaults to 2e-4\n",
    "                max_iters (int, optional): number of max iterations. Defaults to 1000\n",
    "                check_prog (int, optional): checking progress threshold. Defaults to 100\n",
    "                \n",
    "            Returns:\n",
    "                Tuple [list, list, list]: weights, mses, grads\n",
    "        \"\"\"\n",
    "        \n",
    "        results = {'weights': [w],\n",
    "                   'mses': [],\n",
    "                   'grads': []}\n",
    "        \n",
    "        iters = 1\n",
    "        X = np.hstack((np.ones((len(x), 1)), x[:, None]))\n",
    "        dw = np.array(2 * epsilon)\n",
    "        \n",
    "        while abs(dw.sum()) > epsilon and iters <= max_iters:\n",
    "            grad, mse = grad_f(X, y, w)\n",
    "            dw = learn_r * grad\n",
    "            w -= dw \n",
    "            \n",
    "            results['weights'].append(list(w))\n",
    "            results['mses'].append(mse)\n",
    "            results['grads'].append(list(grad))\n",
    "                \n",
    "            iters += 1\n",
    "\n",
    "        print('Got to the min:')\n",
    "        self.print_iters(iters - 1, w)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    \n",
    "    def print_iters(self, iters, w):\n",
    "        print(f'Iter {iters}\\nw0: {w[0]:.2f}\\nw1: {w[1]:.2f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объявление экземпляра класса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd = GradientDescent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение в переменную результатов вычисления градиента:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads_regular = gd.gradient_descent(features, \n",
    "                                    target, \n",
    "                                    w=[-20, -10], \n",
    "                                    grad_f=mse_gradient,\n",
    "                                    learn_r=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика линий уровня с градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_gradient_descent_2d(features, target, \n",
    "                         weights=grads_regular['weights'], \n",
    "                         title='Gradient Descent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Вывод**\n",
    ">\n",
    ">Реализован алгоритм вычисления градиентного спуска."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; opacity: 0.2; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Стохастический градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1. Математическое обоснование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Стохастический градиентный спуск** помогает ускорить обучение модели.\n",
    "\n",
    "Пааметр вычисляется не за счёт использования всего набора данных, а только для его $ i $-х точек:\n",
    "\n",
    "$$ w_{t+1} = w_t - \\mu \\nabla f_i (w_t) = w_t - \\mu \\frac {\\delta}{\\delta w_t} f_i (w_t) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Алгоритм стохастического градиентного спуска по мини-батчам:**\n",
    "\n",
    "1. Подача на вход гиперпараметров: размер батча, количество эпох и величина шага.\n",
    "2. Определение начальных значений весов модели.\n",
    "3. Разбиение обучающей выборки на батчи для каждой эпохи.\n",
    "4. Вычисление градиента функции потерь и обновление весов модели для каждого батча.\n",
    "5. Возвращение последних весов модели.\n",
    "\n",
    "> **Батч** - разбитые на части перемешанные данные выборки.\n",
    "\n",
    "* Число батчей соответствует числу итераций для завершения одной эпохи.\n",
    "* Число эпох зависит от размера обучающей выборки.\n",
    "* Эпоха завершается, когда алгоритм SGD проходит один раз по всем батчам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; opacity: 0.2; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2. Реализация алгоритма"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объявление класса стохастического градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticGradientDescent(GradientDescent):            \n",
    "    \n",
    "    def gradient_descent(self, \n",
    "                         x: [np.array, pd.Series],\n",
    "                         y: [np.array, pd.Series], \n",
    "                         w: list,\n",
    "                         grad_f: Mapping,\n",
    "                         learn_r: float = 0.01,\n",
    "                         max_iters: int = 500,\n",
    "                         descent: str = 'stochastic', \n",
    "                         batch_size: int = 10,\n",
    "                         seed: int = 2020) -> Tuple [list, list, list]:\n",
    "        \n",
    "        \"\"\" Gradient descent\n",
    "        \n",
    "            Args:\n",
    "                x (np.array, pd.Series): features\n",
    "                y (np.array, pd.Series): target\n",
    "                w (list): weights\n",
    "                grad_f (Mapping): gradient function\n",
    "                learn_r (float, optional): learning rate. Defaults to 0.1\n",
    "                max_iters (int, optional): number of max iterations. Defaults to 1000\n",
    "                descent (string, optional): type of SGD. Defaults to 'stochastic'\n",
    "                batch_size (int, optional): batch size. Defaults to 10\n",
    "                seed (int, optional): seed value. Defaults to 2020\n",
    "                \n",
    "            Returns:\n",
    "                Tuple [list, list, list]: weights, mses, grads\n",
    "        \"\"\"\n",
    "        \n",
    "        results = {'weights': [w],\n",
    "                   'mses': [],\n",
    "                   'grads': []}\n",
    "        \n",
    "        iters = 1\n",
    "        X = np.hstack((np.ones((len(x), 1)), x[:, None]))\n",
    "        \n",
    "        if seed is None:\n",
    "            np.random.seed(0)\n",
    "        \n",
    "        while iters <= max_iters:\n",
    "            if descent == 'stochastic':\n",
    "                w, mse, grad = self.stochastic_descent(X, y, w, grad_f, learn_r)\n",
    "            \n",
    "            elif descent == 'minibatch':\n",
    "                w, mse, grad = self.minibatch_descent(X, y, w, grad_f, learn_r, batch_size)\n",
    "            \n",
    "            results['weights'].append(list(w))\n",
    "            results['mses'].append(mse)\n",
    "            results['grads'].append(list(grad))\n",
    "\n",
    "            iters += 1\n",
    "\n",
    "        print('Got to the min:')\n",
    "        self.print_iters(iters - 1, w)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    \n",
    "    def stochastic_descent(self, \n",
    "                           X: np.array,\n",
    "                           y: [np.array, pd.Series], \n",
    "                           w: list,\n",
    "                           grad_f: Mapping,\n",
    "                           learn_r: float) -> Tuple [list, float, list]:\n",
    "        \n",
    "        i = np.random.randint(len(X))\n",
    "        grad, mse = grad_f(X[i, None], y[i, None], w)\n",
    "        w -= learn_r * grad\n",
    "        \n",
    "        return w, mse, grad\n",
    "    \n",
    "    \n",
    "    def minibatch_descent(self, \n",
    "                          X: np.array,\n",
    "                          y: [np.array, pd.Series], \n",
    "                          w: list, \n",
    "                          grad_f: Mapping,\n",
    "                          learn_r: float, \n",
    "                          batch_size: int) -> Tuple [list, float, list]:\n",
    "        \n",
    "        i = np.random.choice(range(len(X)), batch_size, replace=False)\n",
    "        grad, mse = grad_f(X[i], y[i], w)\n",
    "        w -= learn_r * grad\n",
    "        \n",
    "        return w, mse, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объявление экземпляра класса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = StochasticGradientDescent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Стохастический градиентный спуск**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение аргументов вычисления градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd.grads_stochastic_args = {'x': features,\n",
    "                             'y': target,\n",
    "                             'w': [-20, -10], \n",
    "                             'grad_f': mse_gradient,\n",
    "                             'learn_r': 0.01,\n",
    "                             'max_iters': 300,\n",
    "                             'descent': 'stochastic'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение в переменную результатов вычисления градиента:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd.grads_stochastic = sgd.gradient_descent(*sgd.grads_stochastic_args.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика линий уровня с градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_2d(features, target, \n",
    "                         weights=sgd.grads_stochastic['weights'],\n",
    "                         title='Stochastic Gradient Descent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Стохастический градиентный спуск по мини-батчам**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение копии аргументов градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd.grads_minibatch_args = sgd.grads_stochastic_args.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изменение параметров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd.grads_minibatch_args['learn_r'] = 0.01\n",
    "sgd.grads_minibatch_args['max_iters'] = 500\n",
    "sgd.grads_minibatch_args['descent'] = 'minibatch'\n",
    "sgd.grads_minibatch_args['batch_size'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение в переменную результатов вычисления градиента:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sgd.grads_minibatch = sgd.gradient_descent(*sgd.grads_minibatch_args.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика линий уровня с градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_gradient_descent_2d(features, target, \n",
    "                         weights=sgd.grads_minibatch['weights'], \n",
    "                         title='Mini-Batch Gradient Descent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Вывод**\n",
    ">\n",
    ">Реализован алгоритм вычисления стохастического градиентного спуска."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Алгоритмы оптимизации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Математическое обоснование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Оптимизатор** - метод достижения лучших результатов, ускоряющих обучение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Оптимизатор Adam**\n",
    "\n",
    "**Adam** - адптивный метод оптимизации. Сочетает в себе и идею накопления движения и идею более слабого обновления весов для типичных признаков.\n",
    "\n",
    "$$ w_t = w_{t-1} - \\frac {\\mu}{\\sqrt {\\hat v_t + \\epsilon}} \\hat m_t $$\n",
    "\n",
    "* $ \\hat m_t $ - первый момент\n",
    "* $ \\hat v_t $ - второй момент\n",
    "\n",
    "Искусственное увеличение $ m_t, v_t $ на первых шагах:\n",
    "\n",
    "$$ m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t $$ \n",
    "$$ \\hat m_t = \\frac {m_t}{1 - \\beta_1^t} $$\n",
    "\n",
    "$$ v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 $$ \n",
    "$$ \\hat v_t = \\frac {v_t}{1 - \\beta_2^t} $$\n",
    "\n",
    "* $ \\beta_1, \\beta_2 $ - дополнительные параметры (0.9 и 0.999 соответственно)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Импульс Нестерова**"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Импульс Нестерова** - адап\n",
    "\n",
    "![image-2.png](attachment:image-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; opacity: 0.2; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Реализация алгоритма"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объявление класса алгоритмов оптимизации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimization():        \n",
    "        \n",
    "    def gradient_descent_optimization(self,\n",
    "                                      x: np.array,\n",
    "                                      y: np.array, \n",
    "                                      w: list, \n",
    "                                      grad_f: Mapping,\n",
    "                                      learn_r: float = 0.01,\n",
    "                                      max_iters: int = 500,\n",
    "                                      epsilon: float = 1e-8,\n",
    "                                      beta1: float = 0.9,\n",
    "                                      beta2: float = 0.999,\n",
    "                                      optimizer: str = 'adam',\n",
    "                                      seed: int = 2020) -> Tuple [list, list, list]:\n",
    "        \n",
    "        \"\"\" Gradient descent optimization\n",
    "        \n",
    "            Args:\n",
    "                x (np.array): features\n",
    "                y (np.array): target\n",
    "                w (list): weights\n",
    "                grad_f (Mapping): gradient function\n",
    "                learn_r (float, optional): learning rate. Defaults to 0.01\n",
    "                max_iters (int, optional): number of max iterations. Defaults to 500 \n",
    "                epsilon (float, optional): epsilon value. Defaults to 1e-8\n",
    "                beta1 (float, optional): 1st additional parameter. Defaults to 0.9\n",
    "                beta2 (float, optional): 2nd additional parameter. Defaults to 0.999\n",
    "                optimizer (string, optional): optimizer type. Defaults to 'adam'\n",
    "                seed (int, optional): seed value. Defaults to 2020\n",
    "                \n",
    "            Returns:\n",
    "                Tuple [list, list, list]: weights, mses, grads\n",
    "        \"\"\"\n",
    "        \n",
    "        results = {'weights': [w],\n",
    "                   'mses': [],\n",
    "                   'grads': []}\n",
    "        \n",
    "        m, v = 0, 0\n",
    "        iters = 1\n",
    "        \n",
    "        X = np.hstack((np.ones((len(x), 1)), x[:, None]))\n",
    "        \n",
    "        if seed is None:\n",
    "            np.random.seed(0)\n",
    "        \n",
    "        while iters <= max_iters:\n",
    "            if optimizer == 'adam':\n",
    "                grad, mse = grad_f(X, y, w)\n",
    "                \n",
    "                w, m, v = self.adam_optimizer(grad, w, grad_f, learn_r, \n",
    "                                              iters, epsilon, beta1, beta2, m, v)\n",
    "                \n",
    "            results['weights'].append(list(w))\n",
    "            results['mses'].append(mse)\n",
    "            results['grads'].append(list(grad))\n",
    "            \n",
    "            iters += 1\n",
    "        \n",
    "        print('Got to the min:')\n",
    "        self.print_iters(iters - 1, w)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    \n",
    "    def adam_optimizer(self,\n",
    "                       grad: list,\n",
    "                       w: list, \n",
    "                       grad_f: Mapping,\n",
    "                       learn_r: float,\n",
    "                       iters: int,\n",
    "                       epsilon: float,\n",
    "                       beta1: float,\n",
    "                       beta2: float,\n",
    "                       m: float,\n",
    "                       v: float) -> Tuple [list, float, float]:\n",
    "\n",
    "        m = beta1 * m + (1 - beta1) * grad\n",
    "        v = beta2 * v + (1 - beta2) * grad ** 2\n",
    "\n",
    "        m_hat = m / (1 - beta1 ** iters)\n",
    "        v_hat = v / (1 - beta2 ** iters)\n",
    "\n",
    "        w = w - (learn_r * m_hat) / (np.sqrt(v_hat) + epsilon)\n",
    "        \n",
    "        return w, m, v\n",
    "    \n",
    "    \n",
    "    def print_iters(self, iters, w):\n",
    "        print(f'Iter {iters}\\nw0: {w[0]:.2f}\\nw1: {w[1]:.2f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объявление экземпляра класса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Optimization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Вывод**\n",
    ">\n",
    ">Реализованы алгоритмы оптимизации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Искусственные ландшафты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для установления качества работы алгоритмов будут рассмотрены следующие ландшафты, образованные сложными функциями: \n",
    "\n",
    "* Функция Матьяса\n",
    "* Функция Изома\n",
    "* Функция Химмельблау"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание множества точек:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.linspace(-5, 5, 1000)\n",
    "y_axis = np.linspace(-5, 5, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание сетки из множества точек:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = np.meshgrid(x_axis, y_axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; opacity: 0.2; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Функция Матьяса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция Матьяса выглядит следующим образом:\n",
    "\n",
    "$$ f(x, y) = 0.26 (x^2 + y^2) - 0.48xy $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определение экземпляра класса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matias = StochasticGradientDescent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение функции потерь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matias.loss_func = lambda x, y: 0.26 * (x ** 2 + y ** 2) - 0.48 * x * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_function_2d(x_axis, y_axis, \n",
    "                 test_matias.loss_func, \n",
    "                 '2D Matias function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_3d(X, Y, \n",
    "                         loss_f=test_matias.loss_func,\n",
    "                         title='3D Matias function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание дифференцируемой функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matias.deriv_func = 0.26 * (x ** 2 + y ** 2) - 0.48 * x * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран частных производных каждой переменной:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff(test_matias.deriv_func, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff(test_matias.deriv_func, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание функций вычисления частных производных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matias.partial_x = lambda x, y: 0.52 * x - 0.48 * y\n",
    "test_matias.partial_y = lambda x, y: -0.48 * x + 0.52 * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение аргументов вычисления градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matias.grads_args = {'x': test_matias.partial_x(x_axis, y_axis),\n",
    "                          'y': test_matias.partial_y(x_axis, y_axis),\n",
    "                          'w': [-3, 4], \n",
    "                          'grad_f': mse_gradient,\n",
    "                          'learn_r': 0.5, \n",
    "                          'max_iters': 400}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление координат градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_matias.grads = test_matias.gradient_descent(*test_matias.grads_args.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика линий уровня с градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_gradient_descent_function_2d(X, Y, x_axis, y_axis,\n",
    "                                  weights=test_matias.grads['weights'],\n",
    "                                  loss_f=test_matias.loss_func,\n",
    "                                  title='SGD 2D Matias descent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика функции с градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_gradient_descent_3d(X, Y,\n",
    "                         loss_f=test_matias.loss_func,\n",
    "                         weights=test_matias.grads['weights'],\n",
    "                         title='SGD 3D Matias function',\n",
    "                         descent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Оптимизатор Adam**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение копии аргументов градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matias.grads_adam_args = test_matias.grads_args.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изменение параметров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matias.grads_adam_args['learn_r'] = 0.1\n",
    "\n",
    "test_matias.grads_adam_args.update({'epsilon': 1e-8,\n",
    "                                    'beta1': 0.9,\n",
    "                                    'beta2': 0.999,\n",
    "                                    'optimizer': 'adam'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление координат оптимизированного градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matias.grads_adam = optimizer.gradient_descent_optimization(*test_matias.grads_adam_args.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика функции с оптимизированным градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_3d(X, Y,\n",
    "                         loss_f=test_matias.loss_func,\n",
    "                         weights=test_matias.grads_adam['weights'],\n",
    "                         title='SGD Adam 3D Matias function',\n",
    "                         descent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Оптимизатор RMSprop**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение копии аргументов градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matias.grads_rmsprop_args = test_matias.grads_args.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изменение шага обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matias.grads_rmsprop_args['learn_r'] = 0.04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление координат оптимизированного градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matias.grads_rmsprop = optimizer.rmsprop_optimizer(*test_matias.grads_rmsprop_args.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика функции с оптимизированным градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_3d(X, Y,\n",
    "                         loss_f=test_matias.loss_func,\n",
    "                         weights=test_matias.grads_rmsprop['weights'],\n",
    "                         title='SGD Adam 3D Matias function',\n",
    "                         descent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Вывод**\n",
    ">\n",
    ">Реализована проверка работы алгоритмов оптимизации на функции Матьяса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; opacity: 0.2; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Функция Изома"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция Изома выглядит следующим образом:\n",
    "\n",
    "$$ f(x, y) = -cos(x) cos(x) exp \\Bigg(- \\bigg( (x - \\pi)^2 + (y - \\pi)^2 \\bigg) \\Bigg) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определение экземпляра класса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_izom = StochasticGradientDescent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение функции потерь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_izom.loss_func = lambda x, y: -np.cos(x) * np.cos(x) * np.exp(-((x - np.pi) ** 2 + (y - np.pi) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_function_2d(x_axis, y_axis, \n",
    "                 test_izom.loss_func, \n",
    "                 '2D Izom function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_3d(X, Y, \n",
    "                         loss_f=test_izom.loss_func,\n",
    "                         title='3D Izom function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание дифференцируемой функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import cos, exp, pi\n",
    "test_izom.deriv_func = -cos(x) * cos(x) * exp(-((x - pi) ** 2 + (y - pi) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран частных производных каждой переменной:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff(test_izom.deriv_func, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff(test_izom.deriv_func, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание функций вычисления частных производных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_izom.partial_x = lambda x, y: -(-2 * x + 2 * np.pi) * np.exp((-(x - np.pi) ** 2 - (y - np.pi) ** 2)) * \\\n",
    "                                   np.cos(x) ** 2 + 2 * np.exp(-(x - np.pi) ** 2 - (y - np.pi) ** 2) * \\\n",
    "                                   np.sin(x) * np.cos(x)\n",
    "\n",
    "test_izom.partial_y = lambda x, y: -(-2 * y + 2 * np.pi) * np.exp(-(x - np.pi) ** 2 - (y - np.pi) ** 2) * np.cos(x) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение аргументов вычисления градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_izom.grads_args = {'x': test_izom.partial_x(x_axis, y_axis),\n",
    "                        'y': test_izom.partial_y(x_axis, y_axis),\n",
    "                        'w': [-2, 2], \n",
    "                        'grad_f': mse_gradient,\n",
    "                        'learn_r': 0.1, \n",
    "                        'max_iters': 400}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление координат градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_izom.grads = test_izom.gradient_descent(*test_izom.grads_args.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика линий уровня с градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_function_2d(X, Y, x_axis, y_axis,\n",
    "                                  weights=test_izom.grads['weights'],\n",
    "                                  loss_f=test_izom.loss_func,\n",
    "                                  title='SGD 2D Izom descent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика функции потерь с градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_3d(X, Y,\n",
    "                         loss_f=test_izom.loss_func,\n",
    "                         weights=test_izom.grads['weights'],\n",
    "                         title='SGD 3D Izom function',\n",
    "                         descent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Оптимизатор Adam**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение копии аргументов градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_izom.grads_adam_args = test_izom.grads_args.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изменение параметров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_izom.grads_adam_args['learn_r'] = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление координат оптимизированного градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_izom.grads_adam = optimizer.gradient_descent_optimization(*test_izom.grads_adam_args.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика функции с оптимизированным градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_3d(X, Y,\n",
    "                         loss_f=test_izom.loss_func,\n",
    "                         weights=test_izom.grads_adam['weights'],\n",
    "                         title='SGD Adam 3D Izom function',\n",
    "                         descent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Оптимизатор RMSprop**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение копии аргументов градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_izom.grads_rmsprop_args = test_izom.grads_args.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изменение шага обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_izom.grads_rmsprop_args['w'] = [2.5, 3]\n",
    "test_izom.grads_rmsprop_args['learn_r'] = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление координат оптимизированного градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_izom.grads_rmsprop = optimizer.rmsprop_optimizer(*test_izom.grads_rmsprop_args.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика функции с оптимизированным градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_3d(X, Y,\n",
    "                         loss_f=test_izom.loss_func,\n",
    "                         weights=test_izom.grads_rmsprop[0],\n",
    "                         title='SGD RMSprop 3D Izom function',\n",
    "                         descent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Вывод**\n",
    ">\n",
    ">Реализована проверка работы алгоритмов оптимизации на функции Изома."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Функция Химмельблау"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция Химмельбау выглядит следующим образом:\n",
    "\n",
    "$$ f(x, y) = (x^2 + y - 11)^2 + (x + y^2 - 7)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определение экземпляра класса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_himmelblau = StochasticGradientDescent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение функции потерь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_himmelblau.loss_func = lambda x, y: (x ** 2 + y - 11) ** 2 + (x + y ** 2 - 7) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика функции потерь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_function_2d(x_axis, y_axis, \n",
    "                 test_himmelblau.loss_func, \n",
    "                 '2D Himmelblau function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_gradient_descent_3d(X, Y, \n",
    "                         loss_f=test_himmelblau.loss_func,\n",
    "                         title='Himmelblau function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание дифференцируемой функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_himmelblau.deriv_func = (x ** 2 + y - 11) ** 2 + (x + y ** 2 - 7) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран частных производных каждой переменной:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff(test_himmelblau.deriv_func, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff(test_himmelblau.deriv_func, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание функций вычисления частных производных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_himmelblau.partial_x = lambda x, y: 4 * x * (x ** 2 + y - 11) + 2 * x + 2 * y ** 2 - 14\n",
    "test_himmelblau.partial_y = lambda x, y: 2 * x ** 2 + 4 * y * (x + y ** 2 - 7) + 2 * y - 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение аргументов вычисления градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_himmelblau.grads_args = {'x': test_himmelblau.partial_x(x_axis, y_axis),\n",
    "                              'y': test_himmelblau.partial_y(x_axis, y_axis),\n",
    "                              'w': [-0.3, -1], \n",
    "                              'grad_f': mse_gradient,\n",
    "                              'learn_r': 0.00001, \n",
    "                              'max_iters': 800}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление координат градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_himmelblau.grads = test_himmelblau.gradient_descent(*test_himmelblau.grads_args.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика линий уровня с градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_gradient_descent_function_2d(X, Y, x_axis, y_axis,\n",
    "                                  weights=test_himmelblau.grads['weights'],\n",
    "                                  loss_f=test_himmelblau.loss_func,\n",
    "                                  title='SGD 2D Himmelblau descent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика функции с градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_3d(X, Y,\n",
    "                         loss_f=test_himmelblau.loss_func,\n",
    "                         weights=test_himmelblau.grads['weights'],\n",
    "                         title='SGD 3D Himmelblau function',\n",
    "                         descent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Оптимизатор Adam**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение копии аргументов градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_himmelblau.grads_adam_args = test_himmelblau.grads_args.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изменение шага обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_himmelblau.grads_adam_args['learn_r'] = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление координат оптимизированного градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_himmelblau.grads_adam = optimizer.gradient_descent_optimization(*test_himmelblau.grads_adam_args.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика функции с оптимизированным градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_3d(X, Y,\n",
    "                         loss_f=test_himmelblau.loss_func,\n",
    "                         weights=test_himmelblau.grads_adam['weights'],\n",
    "                         title='SGD Adam 3D Himmelblau function',\n",
    "                         descent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_s = 16\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.plot(x_axis, test_himmelblau.loss_func(x_axis, y_axis))\n",
    "\n",
    "weights = np.array(test_himmelblau.grads_adam['weights'])\n",
    "intercepts, slopes = weights[:, 0], weights[:, 1]\n",
    "\n",
    "plt.plot(intercepts, \n",
    "         test_himmelblau.loss_func(intercepts, slopes),\n",
    "         label='gradient')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.title(f'title\\n', fontsize=font_s + 2)\n",
    "\n",
    "plt.xlabel('w0', fontsize=font_s)\n",
    "plt.ylabel('w1', fontsize=font_s)\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Оптимизатор RMSprop**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление координат оптимизированного градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_himmelblau.grads_rmsprop = optimizer.rmsprop_optimizer(*test_himmelblau.grads_optimizer_args.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика функции с оптимизированным градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_3d(X, Y,\n",
    "                         loss_f=test_himmelblau.loss_func,\n",
    "                         weights=test_himmelblau.grads_rmsprop[0],\n",
    "                         title='SGD RMSprop 3D Himmelblau function',\n",
    "                         descent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Вывод**\n",
    ">\n",
    ">Реализована проверка работы алгоритмов оптимизации на функции Химмельблау."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; opacity: 0.5; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Общий вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size: 20px; padding: 15px 0;\">\n",
    "    <a href=\"#Содержание\" data-toc-modified-id=\"Содержание\" style=\"text-decoration: none; color: #296eaa; border: 2px dashed #296eaa; opacity: 0.8; border-radius: 3px; padding: 10px 80px;\">\n",
    "        Наверх к содержанию ↑\n",
    "    </a>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
