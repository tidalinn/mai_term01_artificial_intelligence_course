{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<br>\n",
    "<div class=\"toc\">\n",
    "    <ul class=\"toc-item\">\n",
    "        <li>\n",
    "            <span>\n",
    "                <a href=\"#1.-Постановка-задачи\">\n",
    "                    <span class=\"toc-item-num\">1.&nbsp;&nbsp;</span>\n",
    "                    Постановка задачи\n",
    "                </a>\n",
    "            </span>\n",
    "        </li>\n",
    "        <li>\n",
    "            <span>\n",
    "                <a href=\"#2.-Задача-1:-Одномерный-и-многомерный-градиентный-спуск\">\n",
    "                    <span class=\"toc-item-num\">2.&nbsp;&nbsp;</span>\n",
    "                    Задача 1: Одномерный и многомерный градиентный спуск\n",
    "                </a>\n",
    "            </span>\n",
    "            <ul class=\"toc-item\">\n",
    "                <li>\n",
    "                    <span>\n",
    "                        <a href=\"#2.1.-Математическое-обоснование\">\n",
    "                            <span class=\"toc-item-num\">2.1.&nbsp;&nbsp;</span>\n",
    "                            Математическое обоснование\n",
    "                        </a>\n",
    "                    </span>\n",
    "                </li>\n",
    "                <li>\n",
    "                    <span>\n",
    "                        <a href=\"#2.2.-Реализация-алгоритма-одномерного-градиентного-спуска\">\n",
    "                            <span class=\"toc-item-num\">2.2.&nbsp;&nbsp;</span>\n",
    "                            Реализация алгоритма одномерного градиентного спуска\n",
    "                        </a>\n",
    "                    </span>\n",
    "                    <ul class=\"toc-item\">\n",
    "                        <li>\n",
    "                            <span>\n",
    "                                <a href=\"#2.2.1.-Подготовка-данных\">\n",
    "                                    <span class=\"toc-item-num\">2.2.1.&nbsp;&nbsp;</span>\n",
    "                                    Подготовка данных\n",
    "                                </a>\n",
    "                            </span>\n",
    "                        </li>\n",
    "                        <li>\n",
    "                            <span>\n",
    "                                <a href=\"#2.2.2.-Градиентный спуск\">\n",
    "                                    <span class=\"toc-item-num\">2.2.2.&nbsp;&nbsp;</span>\n",
    "                                    Градиентный спуск\n",
    "                                </a>\n",
    "                            </span>\n",
    "                        </li>\n",
    "                        <li>\n",
    "                            <span>\n",
    "                                <a href=\"#2.2.3.-Локальный-минимум\">\n",
    "                                    <span class=\"toc-item-num\">2.2.3.&nbsp;&nbsp;</span>\n",
    "                                    Локальный минимум\n",
    "                                </a>\n",
    "                            </span>\n",
    "                        </li>\n",
    "                    </ul>\n",
    "                </li>\n",
    "                <li>\n",
    "                    <span>\n",
    "                        <a href=\"#2.3.-Реализация-алгоритма-многомерного-градиентного-спуска\">\n",
    "                            <span class=\"toc-item-num\">2.3.&nbsp;&nbsp;</span>\n",
    "                            Реализация алгоритма многомерного градиентного спуска\n",
    "                        </a>\n",
    "                    </span>\n",
    "                    <ul class=\"toc-item\">\n",
    "                        <li>\n",
    "                            <span>\n",
    "                                <a href=\"#2.3.1.-Подготовка-данных\">\n",
    "                                    <span class=\"toc-item-num\">2.3.1.&nbsp;&nbsp;</span>\n",
    "                                    Подготовка данных\n",
    "                                </a>\n",
    "                            </span>\n",
    "                        </li>\n",
    "                        <li>\n",
    "                            <span>\n",
    "                                <a href=\"#2.3.2.-Частные-производные\">\n",
    "                                    <span class=\"toc-item-num\">2.3.2.&nbsp;&nbsp;</span>\n",
    "                                    Частные производные\n",
    "                                </a>\n",
    "                            </span>\n",
    "                        </li>\n",
    "                        <li>\n",
    "                            <span>\n",
    "                                <a href=\"#2.3.3.-Градиентный-спуск\">\n",
    "                                    <span class=\"toc-item-num\">2.3.3.&nbsp;&nbsp;</span>\n",
    "                                    Градиентный спуск\n",
    "                                </a>\n",
    "                            </span>\n",
    "                        </li>\n",
    "                        <li>\n",
    "                            <span>\n",
    "                                <a href=\"#2.3.4.-Локальный-минимум\">\n",
    "                                    <span class=\"toc-item-num\">2.3.4.&nbsp;&nbsp;</span>\n",
    "                                    Локальный минимум\n",
    "                                </a>\n",
    "                            </span>\n",
    "                        </li>\n",
    "                    </ul>\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>\n",
    "            <span>\n",
    "                <a href=\"#3.-Задача-2:-Стохастический-градиентный-спуск\">\n",
    "                    <span class=\"toc-item-num\">3.&nbsp;&nbsp;</span>\n",
    "                    Задача 2: Стохастический градиентный спуск\n",
    "                </a>\n",
    "            </span>\n",
    "        </li>\n",
    "        <li>\n",
    "            <span>\n",
    "                <a href=\"#4.-Задача-3:-Тестирование-на-искусственных-ландшафтах\">\n",
    "                    <span class=\"toc-item-num\">4.&nbsp;&nbsp;</span>\n",
    "                    Задача 3: Тестирование на искусственных ландшафтах\n",
    "                </a>\n",
    "            </span>\n",
    "        </li>\n",
    "        <li>\n",
    "            <span>\n",
    "                <a href=\"#3.-Общий-вывод\">\n",
    "                    <span class=\"toc-item-num\">3.&nbsp;&nbsp;</span>\n",
    "                    Общий вывод\n",
    "                </a>\n",
    "            </span>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лабораторная работа №1: Gradient Descend\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Постановка задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача 1:** запрограммировать собственную реализацию 1) одномерного и 2) многомерного Градиентного спуска.\n",
    "\n",
    "**Задача 2:** запрограммировать собственную реализацию Стохастического градиентного спуска для 1) моментных и 2) адаптивных классификаций.\n",
    "\n",
    "**Задача 3:** протестировать реализацию алгоритмов многомерного и стохастического градиентных спусков на нескольких искусственных ландшафтах, взятых из статьи [Тестовые функции для оптимизации](https://ru.wikipedia.org/wiki/Тестовые_функции_для_оптимизации).\n",
    "\n",
    "**Данные:** наборы данных будут сгенерированы в виде точек.\n",
    "\n",
    "**Результат:** отобразить полученные результаты на графике."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; opacity: 0.5; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Установка необходимых библиотек:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U sympy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импорт всех необходимых библиотек:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple, Mapping\n",
    "\n",
    "from sympy import diff\n",
    "from sympy.abc import x, y\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from plot_charts import *  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Внутрипроектный модуль `plot_charts` включает в себя следующие функции:\n",
    "\n",
    "* `plot_2d_function` - построение двумерного графика\n",
    "\n",
    "* `plot_animated_2d_chart` - построение анимированного двумерного графика с демонстрацией градиентного спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Задача 1: Одномерный и многомерный градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Математическое обоснование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В математическом анализе под **градиентом** понимается направление наискорейшего локального возрастания функции, а под **антиградиентом** – направление наискорейшего её локального убывания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Метод градиентного спуска** - это способ нахождения локального минимума функции в процессе движения в направлении антиградиента."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В общем случае формула градиентного спуска выглядит следующим образом:\n",
    "\n",
    "$$ \\theta_t = \\theta_{t-1} - \\eta \\Delta E \\Big( \\theta_{t-1} \\Big) = \\theta_{t-1} - \\eta \\sum \\limits_{(x,y \\in D)} \\Delta E \\Big( f(x, \\theta_{t-1}), y \\Big) $$\n",
    "\n",
    "И включает в себя функцию ошибки, которая является суммой ошибок на каждом тренировочном примере:\n",
    "\n",
    "$$ E(\\theta) = \\sum \\limits_{(x,y \\in D)} E \\Big( f(x, \\theta), y \\Big) $$\n",
    "\n",
    "Где:\n",
    "* `θ` - веса модели\n",
    "* `η` - скорость обучения\n",
    "* `D` - набор данных, состоящий из пар `(x, y)`\n",
    "* `x` - признаки\n",
    "* `y` - правильный ответ\n",
    "* `E` - функция ошибки, которую можно подсчитать на каждом примере `E(f(x, θ), y)`\n",
    "* `f(x, θ)` - некоторые предсказания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В чистом градиентном спуске скорость обучения задаётся вручную, и она сильно может повлиять на результат:\n",
    "\n",
    "* Если шаги будут слишком маленькими, обучение будет слишком долгим, и возникает вероятность застрять по дороге в неудачном локальном минимуме.\n",
    "\n",
    "* Если шаги будет слишком большими, можно так и не достичь искомого минимума, постоянно его обходя.\n",
    "\n",
    "Минус подхода в том, что для совершения одного шага градиентного спуска необходимо перебрать всё тренировочное множество."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Одномерный градиентный спуск**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиентный спуск в одномерном случае представляет собой обычную одномерную производную. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Производной функции** в точке `x0` называется предел отношения приращения функции `Δy` к вызвавшему его приращению аргумента `Δx` в этой точке при `Δx → 0`.\n",
    "\n",
    "Таким образом **приращению аргумента** функции соответствует следующая формула:\n",
    "\n",
    "$$ f'(x_0) = \\lim_{x \\to x_0} \\frac{f(x) - f(x_0)}{x - x_0} = \\lim_{x \\to x_0} \\frac{(x_0 + \\Delta x) - f(x_0)}{\\Delta x} = \\lim_{x \\to 0} \\frac{\\Delta f(x)}{\\Delta x} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Многомерный градиентный спуск**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиентный спуск в многомерном случае представляет собой алгоритм с двумя и более переменными. Двигаться только в направлении обратном производной уже не представляется возможным, поскольку не существует единственной производной, которая бы описывала все изменения функции. Это является следствием того, что направление движения точек функции зависит более чем от двух координат.\n",
    "\n",
    "Нахождение локального экстремума заключается в нахождении **частной производной** для каждой переменной по соответствующим осям. Тогда для каждой точке на плоскости станут известны скорость изменения функции в этой точке в каждом из направлений."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Частной производной** назваются функции `z'(x)`, `z'(y)`, которые характеризуют скорость изменения функции `z = f(x, y)` в направлении осей `OX` и `OY` соответственно.\n",
    "\n",
    "$$ f(x, y) → \\frac{\\delta f}{\\delta x}, \\frac{\\delta f}{\\delta y} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом **градиент** - это совокупность частных производных по каждой из независимых переменных (полная производная).\n",
    "\n",
    "$$ \\Delta f(x, y) = \\Bigg[  \\frac{\\delta f}{\\delta x} \\frac{\\delta f}{\\delta y} \\Bigg] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; opacity: 0.5; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Реализация алгоритма одномерного градиентного спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание констант:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RADIUS_UNI = 10\n",
    "SHAPE_UNI = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание функции потерь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function_univariate = lambda x: 3 + x ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание функции вычисления производной:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative_function_univariate = lambda x: 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение в переменные `x_axis` и `y_axis` (тренировочного множества) набора точек и их результатов после применения исходной функции соответственно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.linspace(-RADIUS_UNI, RADIUS_UNI, SHAPE_UNI)\n",
    "y_axis = loss_function_univariate(x_axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран двумерного графика исходной функции, где по оси `X` отложен набор исходных точек, а по оси `Y` - результаты применения к ним функции потерь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_2d_chart(x_axis, y_axis, '3 + x^2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание функции, вычисляющей одномерный градиентный спуск:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_univariate(deriv_f: Mapping, \n",
    "                                start_p: float, \n",
    "                                learn_r: float = 0.1, \n",
    "                                iters: int = 10) -> np.array: \n",
    "    \n",
    "    \"\"\" Collect and calculate gradients for each dot.\n",
    "\n",
    "    Args:\n",
    "        deriv_f (Mapping): derivative function.\n",
    "        start_p (float): start point.\n",
    "        learn_r (float, optional): learning rate. Defaults to 0.1.\n",
    "        iters (int, optional): number of iterations. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        np.array: collection of gradients.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        x = start_p\n",
    "        points_collection = []\n",
    "        \n",
    "        for i in range(iters):\n",
    "            start_p -= learn_r * deriv_f(start_p)\n",
    "                \n",
    "            x = start_p\n",
    "            points_collection.append(x)\n",
    "            \n",
    "        return np.array(points_collection)\n",
    "    \n",
    "    except:\n",
    "        print('Убедитесь в корректности переданных аргументов')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение в переменную `grads` результатов вычисления градиентов с помощью функции `gradient_descent_1d`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = gradient_descent_univariate(start_p=-5, \n",
    "                                    learn_r=0.2, \n",
    "                                    deriv_f=derivative_function_univariate, \n",
    "                                    iters=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3. Локальный минимум"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран значения локального минимума:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Локальный минимум:', max(list(map(lambda x, y: (x, y), grads, loss_function_univariate(grads)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран анимированного графика градиентного спуска функции потерь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_animated_2d_chart(x_axis, y_axis, grads, \n",
    "                       loss_function_univariate, \n",
    "                       derivative_function_univariate, \n",
    "                       main_title='Two-Dimensional Gradient Descent of [3 + x^2]',\n",
    "                       x_range=[-3, 3], \n",
    "                       y_range=[3, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Вывод**\n",
    ">    \n",
    "> Проведена успешная реализация алгоритма одномерного градиентного спуска и демонстрация его работы на графике."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; opacity: 0.5; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Реализация алгоритма многомерного градиентного спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1. Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание констант:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RADIUS_MUL = 4\n",
    "SHAPE_MUL = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание функции потерь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function_multivariate = lambda x, y: x ** 2 + y **2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение в переменные `x_axis` и `y_axis` последовательностей точек (весов) для соответствующих осей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.linspace(-RADIUS_MUL, RADIUS_MUL, SHAPE_MUL)\n",
    "y_axis = np.linspace(-RADIUS_MUL, RADIUS_MUL, SHAPE_MUL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохдание координатной плоскости из тренировочного множества `x_axis` и `y_axis`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis, y_axis = np.meshgrid(x_axis, y_axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран трёхмерного графика, где по осям `X` и `Y` отложены веса исходной модели, а по оси `loss_func(x, y)` - уровень потерь при заданных весах, и точкой `B` в качестве локального минимума:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d_chart(x_axis, y_axis, loss_function_multivariate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение в переменную `start_point` координат начальной точки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_point = (3.3, 3.3, loss_function_multivariate(3.3, 3.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран трёхмерного графика с проставленной начальной точкой `A`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d_chart(x_axis, y_axis, loss_function_multivariate, start_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика контуров (вид сверху):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_contours_chart(x_axis, y_axis, loss_function_multivariate, start_point[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы спуститься из точки `А` в точку `В` необходимо двигаться одновременно по двум осям: `X` и `Y`, спускаясь по оси `loss_func(x, y)`. \n",
    "\n",
    "Для этого необходимо найти частные производные каждой переменной функции потерь для каждой точки `(x, y)` на плоскости, чтобы получить точную скорость изменения функции `loss_func(x, y)` в этой точке в каждом из направлений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2. Частные производные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание дифференцируемой функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "differenciate_function_multivariate = x ** 2 + y ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран частных производных каждой переменной:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Частная производная переменной x:', diff(differenciate_function_multivariate, x))\n",
    "print('Частная производная переменной y:', diff(differenciate_function_multivariate, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку частные производные обеих переменных совпадают, будет задана одна функция вычисления производных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3. Градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание функции вычисления производной:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_function = lambda x: 2 ** x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание функции, вычисляющей многомерный градиентный спуск:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_multivariate(start_p: float, \n",
    "                                  loss_f: Mapping, \n",
    "                                  learn_r: float = 0.1, \n",
    "                                  iters: int = 10) -> Tuple [np.array, np.array, np.array]: \n",
    "    \n",
    "    \"\"\" Collect and calculate gradient for each dot.\n",
    "\n",
    "    Args:\n",
    "        start_p (float): start point coords.\n",
    "        loss_f (Mapping): loss function.\n",
    "        learn_r (float, optional): learning rate. Defaults to 0.1.\n",
    "        iters (int, optional): num of iters. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        Tuple [np.array, np.array, np.array]: \n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        x_list, y_list, l_list = [], [], []\n",
    "        x, y = start_point[0], start_point[1]\n",
    "\n",
    "        for _ in range(iters):\n",
    "            x_list.append(x)\n",
    "            y_list.append(y)\n",
    "            l_list.append(loss_f(x, y))\n",
    "\n",
    "            x = x - learn_r * partial_function(x)\n",
    "            y = y - learn_r * partial_function(y)\n",
    "\n",
    "        return x_list, y_list, l_list\n",
    "    \n",
    "    except:\n",
    "        print('Убедитесь в корректности переданных аргументов')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение в переменные `x_list`, `y_list` и `l_list` результатов вычисления градиентов с помощью функции `gradient_descent_multivariate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_list, y_list, l_list = gradient_descent_multivariate(start_p=start_point, \n",
    "                                                       loss_f=loss_function_multivariate, \n",
    "                                                       learn_r=0.05, \n",
    "                                                       iters=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4. Локальный минимум"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран значения локального минимума:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Локальный минимум:', min(list(map(lambda x, y, z: (x, y, z), x_list, y_list, l_list))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран трёхмерного графика градиентного спуска функции потерь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d_gradient_chart(x=x_axis, \n",
    "                       y=y_axis, \n",
    "                       loss_f=loss_function_multivariate, \n",
    "                       coords_s=start_point, \n",
    "                       grad_weights=[x_list, y_list, l_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран анимированного графика градиентного спуска функции потерь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_animated_3d_chart(x=x_axis, \n",
    "                       y=y_axis, \n",
    "                       loss_f=loss_function_multivariate, \n",
    "                       grad_weights=[x_list, y_list, l_list], \n",
    "                       main_title='Multi-Dimensional Gradient Descent of [x^2 + y^2]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; opacity: 0.5; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Задача 2: Стохастический градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; opacity: 0.5; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Задача 3: Тестирование на искусственных ландшафтах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; opacity: 0.5; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Общий вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size: 20px; padding: 15px 0;\">\n",
    "    <a href=\"#Содержание\" data-toc-modified-id=\"Содержание\" style=\"text-decoration: none; color: #296eaa; border: 2px dashed #296eaa; opacity: 0.8; border-radius: 3px; padding: 10px 80px;\">\n",
    "        Наверх к содержанию ↑\n",
    "    </a>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a2a30c3546c26b196ca13320ca154321d3c7ddf15225431c25ddbdf7ba8fe64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
