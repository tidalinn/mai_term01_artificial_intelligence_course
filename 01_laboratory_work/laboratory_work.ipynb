{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<br>\n",
    "<div class=\"toc\">\n",
    "    <ul class=\"toc-item\">\n",
    "        <li>\n",
    "            <span>\n",
    "                <a href=\"#1.-Импорт-библиотек\">\n",
    "                    <span class=\"toc-item-num\">1.&nbsp;&nbsp;</span>\n",
    "                    Импорт библиотек\n",
    "                </a>\n",
    "            </span>\n",
    "        </li>\n",
    "        <li>\n",
    "            <span>\n",
    "                <a href=\"#2.-Извлечение-данных-из-файла\">\n",
    "                    <span class=\"toc-item-num\">2.&nbsp;&nbsp;</span>\n",
    "                    Извлечение данных из файла\n",
    "                </a>\n",
    "            </span>\n",
    "        </li>\n",
    "        <li style=\"margin-top: 10px\">\n",
    "            <span>\n",
    "                <a href=\"#3.-Алгоритмы-градиентного-спуска\">\n",
    "                    <span class=\"toc-item-num\">3.&nbsp;&nbsp;</span>\n",
    "                    Алгоритмы градиентного спуска\n",
    "                </a>\n",
    "            </span>\n",
    "            <ul class=\"toc-item\">\n",
    "                <li>\n",
    "                    <span>\n",
    "                        <a href=\"#3.1.-Градиентный-спуск\">\n",
    "                            <span class=\"toc-item-num\">3.1.&nbsp;&nbsp;</span>\n",
    "                            Градиентный спуск\n",
    "                        </a>\n",
    "                    </span>\n",
    "                    <ul class=\"toc-item\">\n",
    "                        <li>\n",
    "                            <span>\n",
    "                                <a href=\"#3.1.1.-Математическое-обоснование\">\n",
    "                                    <span class=\"toc-item-num\">3.1.1.&nbsp;&nbsp;</span>\n",
    "                                    Математическое обоснование\n",
    "                                </a>\n",
    "                            </span>\n",
    "                        </li>\n",
    "                        <li>\n",
    "                            <span>\n",
    "                                <a href=\"#3.1.2.-Реализация-алгоритма\">\n",
    "                                    <span class=\"toc-item-num\">3.1.2.&nbsp;&nbsp;</span>\n",
    "                                    Реализация алгоритма\n",
    "                                </a>\n",
    "                            </span>\n",
    "                        </li>\n",
    "                    </ul>\n",
    "                </li>\n",
    "                <li>\n",
    "                    <span>\n",
    "                        <a href=\"#3.2.-Стохастический-градиентный-спуск\">\n",
    "                            <span class=\"toc-item-num\">3.2.&nbsp;&nbsp;</span>\n",
    "                            Стохастический градиентный спуск\n",
    "                        </a>\n",
    "                    </span>\n",
    "                    <ul class=\"toc-item\">\n",
    "                        <li>\n",
    "                            <span>\n",
    "                                <a href=\"#3.2.1.-Математическое-обоснование\">\n",
    "                                    <span class=\"toc-item-num\">3.2.1.&nbsp;&nbsp;</span>\n",
    "                                    Математическое обоснование\n",
    "                                </a>\n",
    "                            </span>\n",
    "                        </li>\n",
    "                        <li>\n",
    "                            <span>\n",
    "                                <a href=\"#3.2.2.-Реализация-алгоритма\">\n",
    "                                    <span class=\"toc-item-num\">3.2.2.&nbsp;&nbsp;</span>\n",
    "                                    Реализация алгоритма\n",
    "                                </a>\n",
    "                            </span>\n",
    "                        </li>\n",
    "                    </ul>\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li style=\"margin-top: 10px\">\n",
    "            <span>\n",
    "                <a href=\"#4.-Алгоритмы-оптимизации\">\n",
    "                    <span class=\"toc-item-num\">4.&nbsp;&nbsp;</span>\n",
    "                    Алгоритмы оптимизации\n",
    "                </a>\n",
    "            </span>\n",
    "            <ul>\n",
    "                <li>\n",
    "                    <span>\n",
    "                        <a href=\"#4.1.-Математическое-обоснование\">\n",
    "                            <span class=\"toc-item-num\">4.1.&nbsp;&nbsp;</span>\n",
    "                            Математическое обоснование\n",
    "                        </a>\n",
    "                    </span>\n",
    "                </li>\n",
    "                <li>\n",
    "                    <span>\n",
    "                        <a href=\"#4.2.-Реализация-алгоритма\">\n",
    "                            <span class=\"toc-item-num\">4.2.&nbsp;&nbsp;</span>\n",
    "                            Реализация алгоритма\n",
    "                        </a>\n",
    "                    </span>\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li style=\"margin-top: 10px\">\n",
    "            <span>\n",
    "                <a href=\"#5.-Искусственные-ландшафты\">\n",
    "                    <span class=\"toc-item-num\">5.&nbsp;&nbsp;</span>\n",
    "                    Искусственные ландшафты\n",
    "                </a>\n",
    "            </span>\n",
    "            <ul>\n",
    "                <li>\n",
    "                    <span>\n",
    "                        <a href=\"#5.1.-Функция-Матьяса\">\n",
    "                            <span class=\"toc-item-num\">5.1.&nbsp;&nbsp;</span>\n",
    "                            Функция Матьяса\n",
    "                        </a>\n",
    "                    </span>\n",
    "                </li>\n",
    "                <li>\n",
    "                    <span>\n",
    "                        <a href=\"#5.2.-Функция-Изома\">\n",
    "                            <span class=\"toc-item-num\">5.2.&nbsp;&nbsp;</span>\n",
    "                            Функция Изома\n",
    "                        </a>\n",
    "                    </span>\n",
    "                </li>\n",
    "                <li>\n",
    "                    <span>\n",
    "                        <a href=\"#5.3.-Функция-Химмельблау\">\n",
    "                            <span class=\"toc-item-num\">5.1.&nbsp;&nbsp;</span>\n",
    "                            Функция Химмельблау\n",
    "                        </a>\n",
    "                    </span>\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li style=\"margin-top: 10px\">\n",
    "            <span>\n",
    "                <a href=\"#6.-Общий-вывод\">\n",
    "                    <span class=\"toc-item-num\">6.&nbsp;&nbsp;</span>\n",
    "                    Общий вывод\n",
    "                </a>\n",
    "            </span>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа №1: Gradient Descend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача:** запрограммировать собственную реализацию Стохастического градиентного спуска для 1) моментного и 2) адаптивного методов.\n",
    "\n",
    "**Тестирование:** протестировать работу алгоритма стохастического градиентного спуска на искусственных ландшафтах, взятых со страницы: [Тестовые функции для оптимизации](https://ru.wikipedia.org/wiki/Тестовые_функции_для_оптимизации).\n",
    "\n",
    "**Источник данных:** набор данных взят с платформы [Kaggle](https://www.kaggle.com/datasets/rounakbanik/pokemon).\n",
    "\n",
    "**Описание данных:** набор данных содержит информацию обо всех семи поколениях 802 Покемонов.\n",
    "\n",
    "---\n",
    "\n",
    "Для реализации поставленных задач из набора данных `datasets/pokemon.csv` будут взяты следующие столбцы:\n",
    "\n",
    "* `defense` - признак объектов\n",
    "* `attack` - целевой признак\n",
    "* `name` - имя Покемона"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Импорт библиотек"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Установка необходимых библиотек:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sympy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импорт всех необходимых библиотек:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Tuple, Mapping\n",
    "\n",
    "from sympy.abc import x, y\n",
    "from sympy import diff\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "from utils.plot_charts import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Внутрипроектный модуль `plot_charts` включает в себя следующие функции:\n",
    "\n",
    "* `plot_2d_chart` - построение двумерного графика\n",
    "\n",
    "* `plot_animated_2d_chart` - построение анимированного двумерного графика с демонстрацией градиентного спуска\n",
    "\n",
    "* `plot_2d_contours_chart` - построение плоского двумерного графика глубины\n",
    "\n",
    "* `plot_3d_chart` - построение трёхмерного графика\n",
    "\n",
    "* `plot_3d_gradient_chart` - построение трёхмерного графика с градиентным спуском\n",
    "\n",
    "* `plot_animated_3d_chart` - построение анимированного трёхмерного графика с демонстрацией градиентного спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Изучение данных из файла "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение набора данных, состоящего из столбцов `name`, `defense` и `attack`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (pd.read_csv('datasets/pokemon.csv', usecols=['name', 'defense', 'attack'], index_col=0)\n",
    "          .reset_index())[['name', 'defense', 'attack']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран полученного набора данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение в переменные признака объектов и целевого признака:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = StandardScaler().fit_transform(data[['defense']]).flatten()\n",
    "target = data['attack'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран распределения признаков объектов по целевым признакам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(features, target, {'title': 'Распределение признаков объектов по целевым признакам', \n",
    "                             'x': 'features', \n",
    "                             'y': 'target'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Алгоритмы градиентного спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1. Математическое обоснование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Градиентный спуск** - это итеративный алгоритм поиска минимума функции потерь. Иначе говоря, это способ нахождения локального минимума функции потерь в процессе движения в направлении антиградиента.\n",
    "\n",
    "> **Градиент** $ \\nabla f(x) $ показывает направление самого быстого роста функции.\n",
    ">\n",
    "> **Антиградиент** $ - \\nabla f(x) $ показывает направление наискорейшего убывания функции.\n",
    "\n",
    "\n",
    "**Градиент векторной функции** - вектор, состоящий из производных ответа по каждому аргументу.\n",
    "\n",
    "$$ \\nabla f(x) = \\Big( \\frac{\\delta f}{\\delta x_1}, \\frac{\\delta f}{\\delta x_2} ... \\frac{\\delta f}{\\delta x_n} \\Big) $$\n",
    "\n",
    "> **Функция потерь** $L(y,a)$ возвращает число потерь от неправильных ответов модели.\n",
    "\n",
    "$$ L(y,a) = \\sum_{i=1}^n (a_i - y_i)^2 $$\n",
    "\n",
    "* $ y $ - правильные ответы\n",
    "* $ a $ - предсказания\n",
    "\n",
    "Обучение через **минимизацию функции потерь** - это набор таких весов линейной регрессии, чтобы предсказания были максимально близки к правильным ответам.\n",
    "\n",
    "$$ w = argmin_w L(y, a) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Распространённой функцией потерь линейной регрессии является **среднеквадратическая ошибка**:\n",
    "\n",
    "$$ f(y, a) = \\frac {1}{n} \\sum_{i=1}^n (a_i - y_i)^2 $$\n",
    "\n",
    "Где предсказания вычисляются посредством **линейной регресии**, демонстрирующей зависимость переменной $ x $ от одной или нескольких других переменных с линейной функцией зависимости:\n",
    "\n",
    "$$ a = w^T x_i $$\n",
    "$$ a = w_0 + w x_i $$\n",
    "\n",
    "* $ w $ - параметры, минимизирующие MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Алгоритм градиентного спуска**:\n",
    "\n",
    "1. В аргументах алгоритма задаётся начальное значение параметра.\n",
    "2. Рассчитывается градиент функции потерь.\n",
    "3. Вычисление нового значения параметра.\n",
    "\n",
    "Алгоритм градиентного спуска завершается, если выполнено хотя бы одно из условий:\n",
    "\n",
    "* Алгоритм прошёл заданное количество итераций.\n",
    "* Размер шага становится меньше заданного порога."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция потерь:\n",
    "\n",
    "$$ f(w) = \\frac {1}{n} \\sum_{i=1}^{n} (w^T x_i - y_i)^2 $$\n",
    "\n",
    "Градиент функции потерь:\n",
    "\n",
    "$$ \\frac {\\delta}{\\delta w} f(w) = \\frac {2}{n} \\sum_{i=1}^{n} (w^T x_i - y_i)^2 x_i $$\n",
    "\n",
    "Новое значение параметра:\n",
    "\n",
    "$$ w_{t+1} = w_t - \\mu \\nabla f(w_t) = w_t - \\mu \\frac {\\delta}{\\delta w} f(w) $$\n",
    "\n",
    "* $ \\mu $ - размер шага обучения (как правило от 0.001 до 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; opacity: 0.2; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2. Реализация алгоритма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_f = np.hstack((np.ones((len(features), 1)), features[:, None]))\n",
    "lambda x, y: 1 / len(new_f) * (new_f.T @ (new_f @ [-20, -10]) - new_f.T @ target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объявление среднеквадратичной функции потерь с одним параметром:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_gradient(X: [np.array, pd.Series], \n",
    "                 y: [np.array, pd.Series], \n",
    "                 w: list) -> Tuple [np.array, list]:\n",
    "    \n",
    "    \"\"\" Get gradient and mse\n",
    "        \n",
    "        Args:\n",
    "            x (np.array, pd.Series): features with ONES column\n",
    "            y (np.array, pd.Series): target\n",
    "            w (list): weights\n",
    "\n",
    "        Returns:\n",
    "            Tuple [np.array, list]: gradients, mse\n",
    "    \"\"\"\n",
    "    \n",
    "    y_hat = X @ w\n",
    "    error = X.T @ y_hat - X.T @ y\n",
    "    grad = (2 / len(X)) * error\n",
    "    mse = (error ** 2).mean()\n",
    "\n",
    "    return grad, mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объявление класса градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent:    \n",
    "    \n",
    "    def gradient_descent(self,\n",
    "                         x: [np.array, pd.Series],\n",
    "                         y: [np.array, pd.Series], \n",
    "                         w: list, \n",
    "                         grad_f: Mapping,\n",
    "                         learn_r: float = 0.1, \n",
    "                         epsilon: float = 2e-4, \n",
    "                         max_iters: int = 1000) -> Tuple [list, list, list]:\n",
    "        \n",
    "        \"\"\" Regular gradient descent\n",
    "        \n",
    "            Args:\n",
    "                x (np.array, pd.Series): features\n",
    "                y (np.array, pd.Series): target\n",
    "                w (list): weights\n",
    "                grad_f (Mapping): gradient descent function\n",
    "                learn_r (float, optional): learning rate. Defaults to 0.1\n",
    "                epsilon (float, optional): epsilon value. Defaults to 2e-4\n",
    "                max_iters (int, optional): number of max iterations. Defaults to 1000\n",
    "                check_prog (int, optional): checking progress threshold. Defaults to 100\n",
    "                \n",
    "            Returns:\n",
    "                Tuple [list, list, list]: weights, mses, grads\n",
    "        \"\"\"\n",
    "        \n",
    "        weights, mses, grads = [w], [], []\n",
    "        \n",
    "        iters = 1\n",
    "        X = np.hstack((np.ones((len(x), 1)), x[:, None]))\n",
    "        dw = np.array(2 * epsilon)\n",
    "        \n",
    "        while abs(dw.sum()) > epsilon and iters <= max_iters:\n",
    "            grad, mse = grad_f(X, y, w)\n",
    "            dw = learn_r * grad\n",
    "            w -= dw \n",
    "            \n",
    "            weights.append(list(w))\n",
    "            mses.append(mse)\n",
    "            grads.append(list(grad))\n",
    "                \n",
    "            iters += 1\n",
    "\n",
    "        print('Got to the min:')\n",
    "        self.print_iters(iters - 1, w)\n",
    "        \n",
    "        return weights, mses, grads\n",
    "    \n",
    "    \n",
    "    def print_iters(self, iters, w):\n",
    "        print(f'Iter {iters}\\nw0: {w[0]:.2f}\\nw1: {w[1]:.2f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объявление экземпляра класса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd = GradientDescent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение в переменную результатов вычисления градиента:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads_regular = gd.gradient_descent(features, \n",
    "                                    target, \n",
    "                                    w=[-20, -10], \n",
    "                                    grad_f=mse_gradient,\n",
    "                                    learn_r=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика линий уровня с градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_gradient_descent_2d(features, target, \n",
    "                         weights=grads_regular[0], \n",
    "                         title='Gradient Descent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Вывод**\n",
    ">\n",
    ">Реализован алгоритм вычисления градиентного спуска."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; opacity: 0.2; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Стохастический градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1. Математическое обоснование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Стохастический градиентный спуск** помогает ускорить обучение модели.\n",
    "\n",
    "Пааметр вычисляется не за счёт использования всего набора данных, а только для его $ i $-х точек:\n",
    "\n",
    "$$ w_{t+1} = w_t - \\mu \\nabla f_i (w_t) = w_t - \\mu \\frac {\\delta}{\\delta w_t} f_i (w_t) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Алгоритм стохастического градиентного спуска по мини-батчам:**\n",
    "\n",
    "1. Подача на вход гиперпараметров: размер батча, количество эпох и величина шага.\n",
    "2. Определение начальных значений весов модели.\n",
    "3. Разбиение обучающей выборки на батчи для каждой эпохи.\n",
    "4. Вычисление градиента функции потерь и обновление весов модели для каждого батча.\n",
    "5. Возвращение последних весов модели.\n",
    "\n",
    "> **Батч** - разбитые на части перемешанные данные выборки.\n",
    "\n",
    "* Число батчей соответствует числу итераций для завершения одной эпохи.\n",
    "* Число эпох зависит от размера обучающей выборки.\n",
    "* Эпоха завершается, когда алгоритм SGD проходит один раз по всем батчам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; opacity: 0.2; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2. Реализация алгоритма"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объявление класса стохастического градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticGradientDescent(GradientDescent):            \n",
    "    \n",
    "    def stochastic_descent(self, \n",
    "                           x: [np.array, pd.Series],\n",
    "                           y: [np.array, pd.Series], \n",
    "                           w: list,\n",
    "                           grad_f: Mapping,\n",
    "                           learn_r: float = 0.1,\n",
    "                           max_iters: int = 500,\n",
    "                           seed: int = 2020) -> Tuple [list, list, list]:\n",
    "        \n",
    "        \"\"\" Stochastic gradient descent\n",
    "        \n",
    "            Args:\n",
    "                x (np.array, pd.Series): features\n",
    "                y (np.array, pd.Series): target\n",
    "                w (list): weights\n",
    "                grad_f (Mapping): gradient function\n",
    "                learn_r (float, optional): learning rate. Defaults to 0.1\n",
    "                max_iters (int, optional): number of max iterations. Defaults to 1000\n",
    "                check_prog (int, optional): checking progress threshold. Defaults to 100\n",
    "                seed (int, optional): seed value. Defaults to 2020\n",
    "                \n",
    "            Returns:\n",
    "                Tuple [list, list, list]: weights, mses, grads\n",
    "        \"\"\"\n",
    "        \n",
    "        ws, mses, grads = [w], [], []\n",
    "        \n",
    "        iters = 1\n",
    "        X = np.hstack((np.ones((len(x), 1)), x[:, None]))\n",
    "        \n",
    "        if seed is None:\n",
    "            np.random.seed(0)\n",
    "        \n",
    "        while iters <= max_iters:\n",
    "            i = np.random.randint(len(X))\n",
    "            grad, mse = grad_f(X[i, None], y[i, None], w)\n",
    "            w -= learn_r * grad\n",
    "            \n",
    "            ws.append(list(w))\n",
    "            mses.append(mse)\n",
    "            grads.append(list(grad))\n",
    "                \n",
    "            iters += 1\n",
    "\n",
    "        print('Got to the min:')\n",
    "        self.print_iters(iters - 1, w)\n",
    "        \n",
    "        return ws, mses, grads\n",
    "    \n",
    "    \n",
    "    def minibatch_descent(self, \n",
    "                          x: [np.array, pd.Series],\n",
    "                          y: [np.array, pd.Series], \n",
    "                          w: list, \n",
    "                          grad_f: Mapping,\n",
    "                          learn_r: float = 0.1, \n",
    "                          batch_size: int = 10, \n",
    "                          max_iters: int = 500,\n",
    "                          seed: int = 2020) -> Tuple [list, list, list]:\n",
    "        \n",
    "        \"\"\" Mini-batch stochastic gradient descent\n",
    "        \n",
    "            Args:\n",
    "                x (np.array, pd.Series): features\n",
    "                y (np.array, pd.Series): target\n",
    "                w (list): weights\n",
    "                grad_f (Mapping): gradient function\n",
    "                learn_r (float, optional): learning rate. Defaults to 0.1\n",
    "                batch_size (int, optional): batch size. Defaults to 10\n",
    "                max_iters (int, optional): number of max iterations. Defaults to 1000\n",
    "                check_prog (int, optional): checking progress threshold. Defaults to 100\n",
    "                seed (int, optional): seed value. Defaults to 2020\n",
    "                \n",
    "            Returns:\n",
    "                Tuple [list, list, list]: weights, mses, grads\n",
    "        \"\"\"\n",
    "        \n",
    "        ws, mses, grads = [w], [], []\n",
    "        \n",
    "        iters = 1\n",
    "        X = np.hstack((np.ones((len(x), 1)), x[:, None]))\n",
    "        \n",
    "        if seed is None:\n",
    "            np.random.seed(0)\n",
    "\n",
    "        while iters <= max_iters:\n",
    "            i = np.random.choice(range(len(X)), batch_size, replace=False)\n",
    "            grad, mse = grad_f(X[i], y[i], w)\n",
    "            w -= learn_r * grad\n",
    "            \n",
    "            ws.append(list(w))\n",
    "            mses.append(mse)\n",
    "            grads.append(list(grad))\n",
    "                \n",
    "            iters += 1\n",
    "\n",
    "        print('Got to the min:')\n",
    "        self.print_iters(iters - 1, w)\n",
    "        \n",
    "        return ws, mses, grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объявление экземпляра класса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = StochasticGradientDescent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Стохастический градиентный спуск**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение в переменную результатов вычисления градиента:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads_stochastic = sgd.stochastic_descent(features, \n",
    "                                          target, \n",
    "                                          w=[-20, -10], \n",
    "                                          grad_f=mse_gradient,\n",
    "                                          learn_r=0.01,\n",
    "                                          max_iters=300,\n",
    "                                          seed=2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика линий уровня с градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_2d(features, target, \n",
    "                         weights=grads_stochastic[0],\n",
    "                         title='Stochastic Gradient Descent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Стохастический градиентный спуск по мини-батчам**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение в переменную результатов вычисления градиента:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grads_minibatch = sgd.minibatch_descent(features, \n",
    "                                        target, \n",
    "                                        w=[-20, -10], \n",
    "                                        grad_f=mse_gradient,\n",
    "                                        learn_r=0.01, \n",
    "                                        batch_size=10, \n",
    "                                        max_iters=300,\n",
    "                                        seed=2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика линий уровня с градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_gradient_descent_2d(features, target, \n",
    "                         weights=grads_minibatch[0], \n",
    "                         title='Mini-Batch Gradient Descent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Вывод**\n",
    ">\n",
    ">Реализован алгоритм вычисления стохастического градиентного спуска."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Алгоритмы оптимизации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Математическое обоснование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Оптимизатор** - метод достижения лучших результатов, ускоряющих обучение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Оптимизатор Adam**\n",
    "\n",
    "**Adam** - моментный метод оптимизации. Сочетает в себе и идею накопления движения и идею более слабого обновления весов для типичных признаков.\n",
    "\n",
    "$$ w_{t+1} = w_{t} - \\frac {\\mu}{\\sqrt {\\hat v_{t}} + \\epsilon} \\hat m_{t} $$\n",
    "\n",
    "* $ \\hat m_t $ - первый момент\n",
    "* $ \\hat v_t $ - второй момент\n",
    "\n",
    "Искусственное увеличение $ m_t, v_t $ на первых шагах:\n",
    "\n",
    "$$ m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t $$ \n",
    "$$ \\hat m_t = \\frac {m_t}{1 - \\beta_1^t} $$\n",
    "\n",
    "$$ v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 $$ \n",
    "$$ \\hat v_t = \\frac {v_t}{1 - \\beta_2^t} $$\n",
    "\n",
    "* $ \\beta_1, \\beta_2 $ - дополнительные параметры (как правило близки к 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Оптимизатор AdaGrad**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AdaGrad** - адаптивный метод оптимизации. Идея алгоритма заключается в том, чтобы использовать что-нибудь, что бы уменьшало обновления для элементов, которые и так часто обновляются."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; opacity: 0.2; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Реализация алгоритма"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объявление класса алгоритмов оптимизации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimization(StochasticGradientDescent):        \n",
    "        \n",
    "    def adam_optimizer(self,\n",
    "                       x: np.array,\n",
    "                       y: np.array, \n",
    "                       w: list, \n",
    "                       grad_f: Mapping,\n",
    "                       learn_r: float = 0.1,\n",
    "                       max_iters: int = 500,\n",
    "                       epsilon: float = 1e-8,\n",
    "                       beta1: float = 0.9,\n",
    "                       beta2: float = 0.999,\n",
    "                       seed: int = 2020) -> Tuple [list, list, list]:\n",
    "        \n",
    "        \"\"\" Adam momentum optimization\n",
    "        \n",
    "            Args:\n",
    "                x (np.array): features\n",
    "                y (np.array): target\n",
    "                w (list): weights\n",
    "                grad_f (Mapping): gradient function\n",
    "                learn_r (float, optional): learning rate. Defaults to 0.1\n",
    "                max_iters (int, optional): number of max iterations. Defaults to 500\n",
    "                epsilon (float, optional): epsilon value. Defaults to 1e-8\n",
    "                beta1 (float, optional): 1st additional parameter. Defaults to 0.9\n",
    "                beta2 (float, optional): 2nd additional parameter. Defaults to 0.999\n",
    "                seed (int, optional): seed value. Defaults to 2020\n",
    "                \n",
    "            Returns:\n",
    "                Tuple [list, list, list]: weights, mses, grads\n",
    "        \"\"\"\n",
    "        \n",
    "        ws, mses, grads = [w], [], []\n",
    "        \n",
    "        ms, vs = [0], [0]\n",
    "        \n",
    "        iters = 1\n",
    "        X = np.hstack((np.ones((len(x), 1)), x[:, None]))\n",
    "        \n",
    "        if seed is None:\n",
    "            np.random.seed(0)\n",
    "        \n",
    "        while iters <= max_iters:\n",
    "            i = np.random.randint(len(X))\n",
    "            grad, mse = grad_f(X[i, None], y[i, None], w)\n",
    "            \n",
    "            m = beta1 * ms[iters-1] + (1 - beta1) * grad\n",
    "            v = beta2 * vs[iters-1] + (1 - beta2) * grad ** 2\n",
    "            \n",
    "            ms.append(m)\n",
    "            vs.append(v)\n",
    "\n",
    "            m_corrected = m / (1 - beta1 ** iters)\n",
    "            v_corrected = v / (1 - beta2 ** iters)\n",
    "            \n",
    "            w_optim = w - learn_r * m_corrected / (np.sqrt(v_corrected) + epsilon)\n",
    "\n",
    "            ws.append(w_optim)\n",
    "            mses.append(mse)\n",
    "            grads.append(list(grad))\n",
    "            \n",
    "            w = w_optim\n",
    "            iters += 1\n",
    "        \n",
    "        print('Got to the min:')\n",
    "        self.print_iters(iters - 1, w)\n",
    "        \n",
    "        return ws, mses, grads\n",
    "    \n",
    "    \n",
    "    def rmsprop_optimizer(self,\n",
    "                          x: np.array,\n",
    "                          y: np.array, \n",
    "                          w: list, \n",
    "                          grad_f: Mapping,\n",
    "                          learn_r: float = 0.1,\n",
    "                          max_iters: int = 500,\n",
    "                          epsilon: float = 1e-8,\n",
    "                          beta: float = 0.9,\n",
    "                          seed: int = 2020) -> Tuple [list, list, list]:\n",
    "        \n",
    "        \"\"\" RMSprop adaptive optimization\n",
    "        \n",
    "            Args:\n",
    "                x (np.array): features\n",
    "                y (np.array): target\n",
    "                w (list): weights\n",
    "                grad_f (Mapping): gradient function\n",
    "                learn_r (float, optional): learning rate. Defaults to 0.1\n",
    "                max_iters (int, optional): number of max iterations. Defaults to 500\n",
    "                epsilon (float, optional): epsilon value. Defaults to 1e-8\n",
    "                beta (float, optional): additional parameter. Defaults to 0.9\n",
    "                seed (int, optional): seed value. Defaults to 2020\n",
    "                \n",
    "            Returns:\n",
    "                Tuple [list, list, list]: weights, mses, grads\n",
    "        \"\"\"\n",
    "        \n",
    "        ws, mses, grads = [w], [], []\n",
    "        \n",
    "        ms = [0]\n",
    "        \n",
    "        iters = 1\n",
    "        X = np.hstack((np.ones((len(x), 1)), x[:, None]))\n",
    "        \n",
    "        if seed is None:\n",
    "            np.random.seed(0)\n",
    "        \n",
    "        while iters <= max_iters:\n",
    "            i = np.random.randint(len(X))\n",
    "            grad, mse = grad_f(X[i, None], y[i, None], w)\n",
    "            \n",
    "            m = beta * ms[iters-1] + (1 - beta) * grad ** 2\n",
    "            \n",
    "            ms.append(m)\n",
    "            \n",
    "            w_optim = w - learn_r * grad / (np.sqrt(m) + epsilon)\n",
    "\n",
    "            ws.append(w)\n",
    "            mses.append(mse)\n",
    "            grads.append(list(grad))\n",
    "            \n",
    "            w = w_optim\n",
    "            iters += 1\n",
    "            \n",
    "        print('Got to the min:')\n",
    "        self.print_iters(iters - 1, w)\n",
    "\n",
    "        return ws, mses, grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объявление экземпляра класса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Optimization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Вывод**\n",
    ">\n",
    ">Реализованы алгоритмы оптимизации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Искусственные ландшафты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для установления качества работы алгоритмов будут рассмотрены следующие ландшафты, образованные сложными функциями: \n",
    "\n",
    "* Функция Матьяса\n",
    "* Функция Изома\n",
    "* Функция Химмельблау"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание множества точек:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.linspace(-5, 5, 1000)\n",
    "y_axis = np.linspace(-5, 5, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание сетки из множества точек:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = np.meshgrid(x_axis, y_axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; opacity: 0.2; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Функция Матьяса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция Матьяса выглядит следующим образом:\n",
    "\n",
    "$$ f(x, y) = 0.26 (x^2 + y^2) - 0.48xy $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определение экземпляра класса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matias = StochasticGradientDescent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение функции потерь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matias.loss_func = lambda x, y: 0.26 * (x ** 2 + y ** 2) - 0.48 * x * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_3d(X, Y, \n",
    "                         loss_f=test_matias.loss_func,\n",
    "                         title='Matias function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание дифференцируемой функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matias.deriv_func = 0.26 * (x ** 2 + y ** 2) - 0.48 * x * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран частных производных каждой переменной:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff(test_matias.deriv_func, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff(test_matias.deriv_func, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание функций вычисления частных производных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matias.partial_x = lambda x, y: 0.52 * x - 0.48 * y\n",
    "test_matias.partial_y = lambda x, y: -0.48 * x + 0.52 * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение аргументов вычисления градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matias.grads_args = {'x': test_matias.partial_x(x_axis, y_axis),\n",
    "                          'y': test_matias.partial_y(x_axis, y_axis),\n",
    "                          'w': [-3, 4], \n",
    "                          'grad_f': mse_gradient,\n",
    "                          'learn_r': 0.5, \n",
    "                          'max_iters': 400}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление координат градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_matias.grads = test_matias.stochastic_descent(*test_matias.grads_args.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика линий уровня с градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_gradient_descent_function_2d(X, Y, x_axis, y_axis,\n",
    "                                  weights=test_matias.grads[0],\n",
    "                                  loss_f=test_matias.loss_func,\n",
    "                                  title='SGD 2D Matias descent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика функции с градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_gradient_descent_3d(X, Y,\n",
    "                         loss_f=test_matias.loss_func,\n",
    "                         weights=test_matias.grads[0],\n",
    "                         title='SGD 3D Matias function',\n",
    "                         descent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение копии аргументов градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matias.grads_optimizer_args = test_matias.grads_args.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изменение шага обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matias.grads_optimizer_args['learn_r'] = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Оптимизатор Adam**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление координат оптимизированного градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matias.grads_adam = optimizer.adam_optimizer(*test_matias.grads_optimizer_args.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика функции с оптимизированным градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_3d(X, Y,\n",
    "                         loss_f=test_matias.loss_func,\n",
    "                         weights=test_matias.grads_adam[0],\n",
    "                         title='SGD Adam 3D Matias function',\n",
    "                         descent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Оптимизатор RMSprop**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление координат оптимизированного градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matias.grads_rmsprop = optimizer.rmsprop_optimizer(*test_matias.grads_optimizer_args.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика функции с оптимизированным градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_3d(X, Y,\n",
    "                         loss_f=test_matias.loss_func,\n",
    "                         weights=test_matias.grads_rmsprop[0],\n",
    "                         title='SGD Adam 3D Matias function',\n",
    "                         descent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Вывод**\n",
    ">\n",
    ">Реализована проверка работы алгоритмов оптимизации на функции Матьяса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; opacity: 0.2; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Функция Изома"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция Изома выглядит следующим образом:\n",
    "\n",
    "$$ f(x, y) = -cos(x) cos(x) exp \\Bigg(- \\bigg( (x - \\pi)^2 + (y - \\pi)^2 \\bigg) \\Bigg) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определение экземпляра класса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_izom = StochasticGradientDescent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение функции потерь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_izom.loss_func = lambda x, y: -np.cos(x) * np.cos(x) * np.exp(-((x - np.pi) ** 2 + (y - np.pi) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_gradient_descent_3d(X, Y, \n",
    "                         loss_f=test_izom.loss_func,\n",
    "                         title='Izom function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание дифференцируемой функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import cos, exp, pi\n",
    "test_izom.deriv_func = -cos(x) * cos(x) * exp(-((x - pi) ** 2 + (y - pi) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран частных производных каждой переменной:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff(test_izom.deriv_func, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff(test_izom.deriv_func, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание функций вычисления частных производных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_izom.partial_x = lambda x, y: -(-2 * x + 2 * np.pi) * np.exp((-(x - np.pi) ** 2 - (y - np.pi) ** 2)) * \\\n",
    "                                   np.cos(x) ** 2 + 2 * np.exp(-(x - np.pi) ** 2 - (y - np.pi) ** 2) * \\\n",
    "                                   np.sin(x) * np.cos(x)\n",
    "\n",
    "test_izom.partial_y = lambda x, y: -(-2 * y + 2 * np.pi) * np.exp(-(x - np.pi) ** 2 - (y - np.pi) ** 2) * np.cos(x) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение аргументов вычисления градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_izom.grads_args = {'x': test_izom.partial_x(x_axis, y_axis),\n",
    "                        'y': test_izom.partial_y(x_axis, y_axis),\n",
    "                        'w': [-2, 2], \n",
    "                        'grad_f': mse_gradient,\n",
    "                        'learn_r': 0.5, \n",
    "                        'max_iters': 400}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление координат градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_izom.grads = test_izom.stochastic_descent(*test_izom.grads_args.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика линий уровня с градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_function_2d(X, Y, x_axis, y_axis,\n",
    "                                  weights=test_izom.grads[0],\n",
    "                                  loss_f=test_izom.loss_func,\n",
    "                                  title='SGD 2D Izom descent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика функции потерь с градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_3d(X, Y,\n",
    "                         loss_f=test_izom.loss_func,\n",
    "                         weights=test_izom.grads[0],\n",
    "                         title='SGD 3D Izom function',\n",
    "                         descent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение копии аргументов градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_izom.grads_optimizer_args = test_izom.grads_args.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изменение шага обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_izom.grads_optimizer_args['learn_r'] = 0.7\n",
    "test_izom.grads_optimizer_args['max_iters'] = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Оптимизатор Adam**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление координат оптимизированного градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_izom.grads_adam = optimizer.adam_optimizer(*test_izom.grads_optimizer_args.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика функции с оптимизированным градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_3d(X, Y,\n",
    "                         loss_f=test_izom.loss_func,\n",
    "                         weights=test_izom.grads_adam[0],\n",
    "                         title='SGD Adam 3D Izom function',\n",
    "                         descent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Оптимизатор RMSprop**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление координат оптимизированного градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_izom.grads_rmsprop = optimizer.rmsprop_optimizer(*test_izom.grads_optimizer_args.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика функции с оптимизированным градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_3d(X, Y,\n",
    "                         loss_f=test_izom.loss_func,\n",
    "                         weights=test_izom.grads_rmsprop[0],\n",
    "                         title='SGD RMSprop 3D Izom function',\n",
    "                         descent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Вывод**\n",
    ">\n",
    ">Реализована проверка работы алгоритмов оптимизации на функции Изома."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Функция Химмельблау"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция Химмельбау выглядит следующим образом:\n",
    "\n",
    "$$ f(x, y) = (x^2 + y - 11)^2 + (x + y^2 - 7)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определение экземпляра класса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_himmelblau = StochasticGradientDescent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение функции потерь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_himmelblau.loss_func = lambda x, y: (x ** 2 + y - 11) ** 2 + (x + y ** 2 - 7) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика функции потерь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_gradient_descent_3d(X, Y, \n",
    "                         loss_f=test_himmelblau.loss_func,\n",
    "                         title='Himmelblau function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание дифференцируемой функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_himmelblau.deriv_func = (x ** 2 + y - 11) ** 2 + (x + y ** 2 - 7) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран частных производных каждой переменной:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff(test_himmelblau.deriv_func, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff(test_himmelblau.deriv_func, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание функций вычисления частных производных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_himmelblau.partial_x = lambda x, y: 4 * x * (x ** 2 + y - 11) + 2 * x + 2 * y ** 2 - 14\n",
    "test_himmelblau.partial_y = lambda x, y: 2 * x ** 2 + 4 * y * (x + y ** 2 - 7) + 2 * y - 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение аргументов вычисления градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_himmelblau.grads_args = {'x': test_himmelblau.partial_x(x_axis, y_axis),\n",
    "                        'y': test_himmelblau.partial_y(x_axis, y_axis),\n",
    "                        'w': [-1, 4.2], \n",
    "                        'grad_f': mse_gradient,\n",
    "                        'learn_r': 0.00001, \n",
    "                        'max_iters': 600}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление координат градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_himmelblau.grads = test_himmelblau.stochastic_descent(*test_himmelblau.grads_args.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика линий уровня с градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_gradient_descent_function_2d(X, Y, x_axis, y_axis,\n",
    "                                  weights=test_himmelblau.grads[0],\n",
    "                                  loss_f=test_himmelblau.loss_func,\n",
    "                                  title='SGD 2D Himmelblau descent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика функции с градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_3d(X, Y,\n",
    "                         loss_f=test_himmelblau.loss_func,\n",
    "                         weights=test_himmelblau.grads[0],\n",
    "                         title='SGD 3D Himmelblau function',\n",
    "                         descent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение копии аргументов градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_himmelblau.grads_optimizer_args = test_himmelblau.grads_args.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изменение шага обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_himmelblau.grads_optimizer_args['learn_r'] = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Оптимизатор Adam**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление координат оптимизированного градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_himmelblau.grads_adam = optimizer.adam_optimizer(*test_himmelblau.grads_optimizer_args.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика функции с оптимизированным градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_3d(X, Y,\n",
    "                         loss_f=test_himmelblau.loss_func,\n",
    "                         weights=test_himmelblau.grads_adam[0],\n",
    "                         title='SGD Adam 3D Himmelblau function',\n",
    "                         descent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Оптимизатор RMSprop**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление координат оптимизированного градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_himmelblau.grads_rmsprop = optimizer.rmsprop_optimizer(*test_himmelblau.grads_optimizer_args.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика функции с оптимизированным градиентным спуском:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent_3d(X, Y,\n",
    "                         loss_f=test_himmelblau.loss_func,\n",
    "                         weights=test_himmelblau.grads_rmsprop[0],\n",
    "                         title='SGD RMSprop 3D Himmelblau function',\n",
    "                         descent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Вывод**\n",
    ">\n",
    ">Реализована проверка работы алгоритмов оптимизации на функции Химмельблау."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; opacity: 0.5; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; opacity: 0.5; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Общий вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size: 20px; padding: 15px 0;\">\n",
    "    <a href=\"#Содержание\" data-toc-modified-id=\"Содержание\" style=\"text-decoration: none; color: #296eaa; border: 2px dashed #296eaa; opacity: 0.8; border-radius: 3px; padding: 10px 80px;\">\n",
    "        Наверх к содержанию ↑\n",
    "    </a>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
