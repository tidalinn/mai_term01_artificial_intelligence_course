{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<br>\n",
    "<div class=\"toc\">\n",
    "    <ul class=\"toc-item\">\n",
    "        <li>\n",
    "            <span>\n",
    "                <a href=\"#1.-Постановка-задачи\">\n",
    "                    <span class=\"toc-item-num\">1.&nbsp;&nbsp;</span>\n",
    "                    Постановка задачи\n",
    "                </a>\n",
    "            </span>\n",
    "        </li>\n",
    "        <li>\n",
    "            <span>\n",
    "                <a href=\"#2.-Задача-1:-Одномерный-и-многомерный-градиентный-спуск\">\n",
    "                    <span class=\"toc-item-num\">2.&nbsp;&nbsp;</span>\n",
    "                    Задача 1: Одномерный и многомерный градиентный спуск\n",
    "                </a>\n",
    "            </span>\n",
    "            <ul class=\"toc-item\">\n",
    "                <li>\n",
    "                    <span>\n",
    "                        <a href=\"#2.1.-Математическое-обоснование\">\n",
    "                            <span class=\"toc-item-num\">2.1.&nbsp;&nbsp;</span>\n",
    "                            Математическое обоснование\n",
    "                        </a>\n",
    "                    </span>\n",
    "                </li>\n",
    "                <li>\n",
    "                    <span>\n",
    "                        <a href=\"#2.2.-Реализация-алгоритма-одномерного-градиентного-спуска\">\n",
    "                            <span class=\"toc-item-num\">2.2.&nbsp;&nbsp;</span>\n",
    "                            Реализация алгоритма одномерного градиентного спуска\n",
    "                        </a>\n",
    "                    </span>\n",
    "                    <ul class=\"toc-item\">\n",
    "                        <li>\n",
    "                            <span>\n",
    "                                <a href=\"#2.2.1.-Подготовка-данных\">\n",
    "                                    <span class=\"toc-item-num\">2.2.1.&nbsp;&nbsp;</span>\n",
    "                                    Подготовка данных\n",
    "                                </a>\n",
    "                            </span>\n",
    "                        </li>\n",
    "                        <li>\n",
    "                            <span>\n",
    "                                <a href=\"#2.2.2.-Градиентный-спуск\">\n",
    "                                    <span class=\"toc-item-num\">2.2.2.&nbsp;&nbsp;</span>\n",
    "                                    Градиентный спуск\n",
    "                                </a>\n",
    "                            </span>\n",
    "                        </li>\n",
    "                    </ul>\n",
    "                </li>\n",
    "                <li>\n",
    "                    <span>\n",
    "                        <a href=\"#2.3.-Реализация-алгоритма-многомерного-градиентного-спуска\">\n",
    "                            <span class=\"toc-item-num\">2.3.&nbsp;&nbsp;</span>\n",
    "                            Реализация алгоритма многомерного градиентного спуска\n",
    "                        </a>\n",
    "                    </span>\n",
    "                    <ul class=\"toc-item\">\n",
    "                        <li>\n",
    "                            <span>\n",
    "                                <a href=\"#2.3.1.-Подготовка-данных\">\n",
    "                                    <span class=\"toc-item-num\">2.3.1.&nbsp;&nbsp;</span>\n",
    "                                    Подготовка данных\n",
    "                                </a>\n",
    "                            </span>\n",
    "                        </li>\n",
    "                        <li>\n",
    "                            <span>\n",
    "                                <a href=\"#2.3.2.-Частные-производные\">\n",
    "                                    <span class=\"toc-item-num\">2.3.2.&nbsp;&nbsp;</span>\n",
    "                                    Частные производные\n",
    "                                </a>\n",
    "                            </span>\n",
    "                        </li>\n",
    "                        <li>\n",
    "                            <span>\n",
    "                                <a href=\"#2.3.3.-Градиентный-спуск\">\n",
    "                                    <span class=\"toc-item-num\">2.3.3.&nbsp;&nbsp;</span>\n",
    "                                    Градиентный спуск\n",
    "                                </a>\n",
    "                            </span>\n",
    "                        </li>\n",
    "                        <li>\n",
    "                            <span>\n",
    "                                <a href=\"#2.3.4.-Тестирование-на-искусственных-ландшафтах\">\n",
    "                                    <span class=\"toc-item-num\">2.3.4.&nbsp;&nbsp;</span>\n",
    "                                    Тестирование на искусственных ландшафтах\n",
    "                                </a>\n",
    "                            </span>\n",
    "                            <ul class=\"toc-item\">\n",
    "                                <li>\n",
    "                                    <span>\n",
    "                                        <a href=\"#2.3.4.1.-Функция-Матьяса\">\n",
    "                                            <span class=\"toc-item-num\">2.3.4.1.&nbsp;&nbsp;</span>\n",
    "                                            Функция Матьяса\n",
    "                                        </a>\n",
    "                                    </span>\n",
    "                                </li>\n",
    "                                <li>\n",
    "                                    <span>\n",
    "                                        <a href=\"#2.3.4.2.-Функция-Изома\">\n",
    "                                            <span class=\"toc-item-num\">2.3.4.2.&nbsp;&nbsp;</span>\n",
    "                                            Функция Изома\n",
    "                                        </a>\n",
    "                                    </span>\n",
    "                                </li>\n",
    "                                <li>\n",
    "                                    <span>\n",
    "                                        <a href=\"#2.3.4.3.-Функция-Била\">\n",
    "                                            <span class=\"toc-item-num\">2.3.4.3.&nbsp;&nbsp;</span>\n",
    "                                            Функция Била\n",
    "                                        </a>\n",
    "                                    </span>\n",
    "                                </li>\n",
    "                                <li>\n",
    "                                    <span>\n",
    "                                        <a href=\"#2.3.4.4.-Функция-Химмельблау\">\n",
    "                                            <span class=\"toc-item-num\">2.3.4.4.&nbsp;&nbsp;</span>\n",
    "                                            Функция Химмельблау\n",
    "                                        </a>\n",
    "                                    </span>\n",
    "                                </li>\n",
    "                            </ul>\n",
    "                        </li>\n",
    "                    </ul>\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li>\n",
    "            <span>\n",
    "                <a href=\"#3.-Задача-2:-Стохастический-градиентный-спуск\">\n",
    "                    <span class=\"toc-item-num\">3.&nbsp;&nbsp;</span>\n",
    "                    Задача 2: Стохастический градиентный спуск\n",
    "                </a>\n",
    "            </span>\n",
    "        </li>\n",
    "        <li>\n",
    "            <span>\n",
    "                <a href=\"#4.-Задача-3:-Тестирование-на-искусственных-ландшафтах\">\n",
    "                    <span class=\"toc-item-num\">4.&nbsp;&nbsp;</span>\n",
    "                    Задача 3: Тестирование на искусственных ландшафтах\n",
    "                </a>\n",
    "            </span>\n",
    "        </li>\n",
    "        <li>\n",
    "            <span>\n",
    "                <a href=\"#3.-Общий-вывод\">\n",
    "                    <span class=\"toc-item-num\">3.&nbsp;&nbsp;</span>\n",
    "                    Общий вывод\n",
    "                </a>\n",
    "            </span>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лабораторная работа №1: Gradient Descend\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Постановка задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача 1:** запрограммировать собственную реализацию 1) одномерного и 2) многомерного Градиентного спуска.\n",
    "\n",
    "**Задача 2:** запрограммировать собственную реализацию Стохастического градиентного спуска для 1) моментных и 2) адаптивных классификаций.\n",
    "\n",
    "При этом протестировать реализацию алгоритмов многомерного и стохастического градиентных спусков на нескольких искусственных ландшафтах, взятых со страницы: [Тестовые функции для оптимизации](https://ru.wikipedia.org/wiki/Тестовые_функции_для_оптимизации).\n",
    "\n",
    "**Данные:** наборы данных будут сгенерированы в виде точек.\n",
    "\n",
    "**Результат:** отобразить полученные результаты на графике."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; opacity: 0.5; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Установка необходимых библиотек:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U sympy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импорт всех необходимых библиотек:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple, Mapping\n",
    "\n",
    "import sympy\n",
    "from sympy import diff, sqrt, cos, sin, exp, pi\n",
    "from sympy.abc import x, y\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import inspect\n",
    "from plot_charts import *  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Внутрипроектный модуль `plot_charts` включает в себя следующие функции:\n",
    "\n",
    "* `plot_2d_function` - построение двумерного графика\n",
    "\n",
    "* `plot_animated_2d_chart` - построение анимированного двумерного графика с демонстрацией градиентного спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Задача 1: Одномерный и многомерный градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Математическое обоснование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В математическом анализе под **градиентом** понимается направление наискорейшего локального возрастания функции, а под **антиградиентом** – направление наискорейшего её локального убывания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Метод градиентного спуска** - это способ нахождения локального минимума функции в процессе движения в направлении антиградиента."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В общем случае формула градиентного спуска выглядит следующим образом:\n",
    "\n",
    "$$ \\theta_t = \\theta_{t-1} - \\eta \\Delta E \\Big( \\theta_{t-1} \\Big) = \\theta_{t-1} - \\eta \\sum \\limits_{(x,y \\in D)} \\Delta E \\Big( f(x, \\theta_{t-1}), y \\Big) $$\n",
    "\n",
    "И включает в себя функцию ошибки, которая является суммой ошибок на каждом тренировочном примере:\n",
    "\n",
    "$$ E(\\theta) = \\sum \\limits_{(x,y \\in D)} E \\Big( f(x, \\theta), y \\Big) $$\n",
    "\n",
    "Где:\n",
    "* `θ` - веса модели\n",
    "* `η` - скорость обучения\n",
    "* `D` - набор данных, состоящий из пар `(x, y)`\n",
    "* `x` - признаки\n",
    "* `y` - правильный ответ\n",
    "* `E` - функция ошибки, которую можно подсчитать на каждом примере `E(f(x, θ), y)`\n",
    "* `f(x, θ)` - некоторые предсказания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В чистом градиентном спуске скорость обучения задаётся вручную, и она сильно может повлиять на результат:\n",
    "\n",
    "* Если шаги будут слишком маленькими, обучение будет слишком долгим, и возникает вероятность застрять по дороге в неудачном локальном минимуме.\n",
    "\n",
    "* Если шаги будет слишком большими, можно так и не достичь искомого минимума, постоянно его обходя.\n",
    "\n",
    "Минус подхода в том, что для совершения одного шага градиентного спуска необходимо перебрать всё тренировочное множество."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Одномерный градиентный спуск**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиентный спуск в одномерном случае представляет собой обычную одномерную производную. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Производной функции** в точке `x0` называется предел отношения приращения функции `Δy` к вызвавшему его приращению аргумента `Δx` в этой точке при `Δx → 0`.\n",
    "\n",
    "Таким образом **приращению аргумента** функции соответствует следующая формула:\n",
    "\n",
    "$$ f'(x_0) = \\lim_{x \\to x_0} \\frac{f(x) - f(x_0)}{x - x_0} = \\lim_{x \\to x_0} \\frac{(x_0 + \\Delta x) - f(x_0)}{\\Delta x} = \\lim_{x \\to 0} \\frac{\\Delta f(x)}{\\Delta x} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Многомерный градиентный спуск**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиентный спуск в многомерном случае представляет собой алгоритм с двумя и более переменными. Двигаться только в направлении обратном производной уже не представляется возможным, поскольку не существует единственной производной, которая бы описывала все изменения функции. Это является следствием того, что направление движения точек функции зависит более чем от двух координат.\n",
    "\n",
    "Нахождение локального экстремума заключается в нахождении **частной производной** для каждой переменной по соответствующим осям. Тогда для каждой точке на плоскости станут известны скорость изменения функции в этой точке в каждом из направлений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Частной производной** назваются функции `z'(x)`, `z'(y)`, которые характеризуют скорость изменения функции `z = f(x, y)` в направлении осей `OX` и `OY` соответственно.\n",
    "\n",
    "$$ f(x, y) → \\frac{\\delta f}{\\delta x}, \\frac{\\delta f}{\\delta y} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом **градиент** - это совокупность частных производных по каждой из независимых переменных (полная производная).\n",
    "\n",
    "$$ \\Delta f(x, y) = \\Bigg[  \\frac{\\delta f}{\\delta x} \\frac{\\delta f}{\\delta y} \\Bigg] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; opacity: 0.5; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Реализация алгоритма одномерного градиентного спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание констант:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RADIUS_UNI = 10\n",
    "SHAPE_UNI = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание функции потерь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function_univariate = lambda x: 3 + x ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание функции вычисления производной:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative_function_univariate = lambda x: 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение в переменные `x_axis` и `y_axis` (тренировочного множества) набора точек и их результатов после применения исходной функции соответственно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.linspace(-RADIUS_UNI, RADIUS_UNI, SHAPE_UNI)\n",
    "y_axis = loss_function_univariate(x_axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран двумерного графика исходной функции, где по оси `X` отложен набор исходных точек, а по оси `Y` - результаты применения к ним функции потерь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_2d_chart(x_axis, y_axis, '3 + x^2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание функции, вычисляющей одномерный градиентный спуск:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_univariate(deriv_f: Mapping, \n",
    "                                start_p: float, \n",
    "                                learn_r: float = 0.1, \n",
    "                                iters: int = 10) -> np.array: \n",
    "    \n",
    "    \"\"\" Collect and calculate gradients for each dot\n",
    "\n",
    "    Args:\n",
    "        deriv_f (Mapping): derivative function\n",
    "        start_p (float): start point\n",
    "        learn_r (float, optional): learning rate. Defaults to 0.1\n",
    "        iters (int, optional): number of iterations. Defaults to 10\n",
    "\n",
    "    Returns:\n",
    "        np.array: collection of gradients\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        x = start_p\n",
    "        points_collection = []\n",
    "        \n",
    "        for i in range(iters):\n",
    "            start_p -= learn_r * deriv_f(start_p)\n",
    "                \n",
    "            x = start_p\n",
    "            points_collection.append(x)\n",
    "            \n",
    "        return np.array(points_collection)\n",
    "    \n",
    "    except:\n",
    "        print('Убедитесь в корректности переданных аргументов')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение в переменную `grads` результатов вычисления градиентов с помощью функции `gradient_descent_1d`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = gradient_descent_univariate(start_p=-5, \n",
    "                                    learn_r=0.2, \n",
    "                                    deriv_f=derivative_function_univariate, \n",
    "                                    iters=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран значения локального минимума:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Локальный минимум:', max(list(map(lambda x, y: (x, y), grads, loss_function_univariate(grads)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран анимированного графика градиентного спуска функции потерь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_animated_2d_chart(x_axis, y_axis, grads, \n",
    "                       loss_function_univariate, \n",
    "                       derivative_function_univariate, \n",
    "                       main_title='Two-Dimensional Gradient Descent of [3 + x^2]',\n",
    "                       x_range=[-3, 3], \n",
    "                       y_range=[3, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Вывод**\n",
    ">    \n",
    "> Проведена успешная реализация алгоритма одномерного градиентного спуска и демонстрация его работы на графике."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; opacity: 0.5; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Реализация алгоритма многомерного градиентного спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создание класса `GradientDescent` со всеми необходимыми для вычисления многомерного градиентного спуска методами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent:\n",
    "    # class inner variables\n",
    "    loss_function = Mapping\n",
    "    differenciate_function = Mapping\n",
    "    partial_x = Mapping\n",
    "    partial_y = Mapping\n",
    "    start_point = ()\n",
    "    \n",
    "    \n",
    "    def diff_sympy(self, *args):\n",
    "        \"\"\" Calculate differencials of loss functions by a passed variable\n",
    "\n",
    "        Args:\n",
    "            *args: variable (x or y)\n",
    "\n",
    "        Returns:\n",
    "            Result of sympy diff function\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            return diff(self.differenciate_function, *args)\n",
    "        \n",
    "        except:\n",
    "            print('Убедитесь в корректности переданных аргументов')\n",
    "    \n",
    "    \n",
    "    def gradient_descent(self,\n",
    "                         learn_r: float = 0.1, \n",
    "                         iters: int = 10) -> Tuple [np.array, np.array, np.array]: \n",
    "\n",
    "        \"\"\" Collect and calculate gradient for each dot\n",
    "\n",
    "        Args:\n",
    "            learn_r (float, optional): learning rate. Defaults to 0.1\n",
    "            iters (int, optional): num of iters. Defaults to 10\n",
    "\n",
    "        Returns:\n",
    "            Tuple [np.array, np.array, np.array]: x_list, y_list, l_list\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            self.learn_r = learn_r\n",
    "            self.iters = iters\n",
    "\n",
    "            x_list, y_list, l_list = [], [], []\n",
    "            x, y = self.start_point[0], self.start_point[1]\n",
    "\n",
    "            for _ in range(iters):\n",
    "                x_list.append(x)\n",
    "                y_list.append(y)\n",
    "                l_list.append(self.loss_function(x, y))\n",
    "\n",
    "                x, y = self.check_partitial_order(x, y, learn_r)\n",
    "\n",
    "            return x_list, y_list, l_list\n",
    "        \n",
    "        except:\n",
    "            print('Убедитесь в корректности переданных аргументов')\n",
    "    \n",
    "    \n",
    "    def check_partitial_order(self, \n",
    "                              x: [float, int], \n",
    "                              y: [float, int],\n",
    "                              learn_r: float) -> Tuple:\n",
    "        \n",
    "        \"\"\" Check number of passed variables\n",
    "\n",
    "        Args:\n",
    "            x ([float, int]): learning rate\n",
    "            y ([float, int]): num of iters\n",
    "            learn_r (float): learning rate\n",
    "\n",
    "        Returns:\n",
    "            Tuple: x, y\n",
    "        \"\"\"\n",
    "        \n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.learn_r = learn_r\n",
    "        \n",
    "        if all(var in str(self.diff_sympy(sympy.abc.x)) for var in ['x', 'y']):\n",
    "            x = x - learn_r * self.partial_x(x, y)\n",
    "            y = y - learn_r * self.partial_y(x, y)\n",
    "        \n",
    "        else:\n",
    "            x = x - learn_r * self.partial_x(x)\n",
    "            y = y - learn_r * self.partial_y(y)\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание функции, выводящей на экран координату локального минимума:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_local_min(f: Mapping, grad_weights: Tuple):\n",
    "    \"\"\" Print local minimum\n",
    "\n",
    "    Args:\n",
    "        f (Mapping): max or min function\n",
    "        grad_weights (Tuple): x_list, y_list, l_list\n",
    "    \n",
    "    Result:\n",
    "        Print string\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        print('Локальный минимум:', f(list(map(lambda x, y, z: (x, y, z), *grad_weights))))\n",
    "    \n",
    "    except:\n",
    "            print('Убедитесь в корректности переданных аргументов')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1. Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание констант:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RADIUS_MUL = 4\n",
    "SHAPE_MUL = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение в переменные `x_axis` и `y_axis` последовательностей точек (весов) для соответствующих осей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.linspace(-RADIUS_MUL, RADIUS_MUL, SHAPE_MUL)\n",
    "y_axis = np.linspace(-RADIUS_MUL, RADIUS_MUL, SHAPE_MUL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохдание координатной плоскости из тренировочного множества `x_axis` и `y_axis`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis, y_axis = np.meshgrid(x_axis, y_axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определение экземпляра класса `GradientDescent`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multivariate_func = GradientDescent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение функции потерь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multivariate_func.loss_function = lambda x, y: x ** 2 + y **2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определение экземпляра класса `GradientDescent`:\n",
    "Сохранение функции потерь:\n",
    "Задание координат начальной точки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multivariate_func.start_point = (3.3, 3.3, 21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран трёхмерного графика, где по осям `X` и `Y` отложены веса исходной модели, а по оси `loss_func(x, y)` - уровень потерь при заданных весах, и точкой `B` в качестве локального минимума:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d_chart(x_axis, y_axis, \n",
    "              multivariate_func.loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран трёхмерного графика с проставленной начальной точкой `A`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d_chart(x_axis, y_axis, \n",
    "              multivariate_func.loss_function, \n",
    "              multivariate_func.start_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран графика контуров (вид сверху):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_contours_chart(x_axis, y_axis,\n",
    "                       multivariate_func.loss_function, \n",
    "                       multivariate_func.start_point[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы спуститься из точки `А` в точку `В` необходимо двигаться одновременно по двум осям: `X` и `Y`, спускаясь по оси `loss_func(x, y)`. \n",
    "\n",
    "Для этого необходимо найти частные производные каждой переменной функции потерь для каждой точки `(x, y)` на плоскости, чтобы получить точную скорость изменения функции `loss_func(x, y)` в этой точке в каждом из направлений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2. Частные производные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание дифференцируемой функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multivariate_func.differenciate_function = x ** 2 + y ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран частных производных каждой переменной:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multivariate_func.diff_sympy(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multivariate_func.diff_sympy(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку частные производные обеих переменных совпадают, будет задана одна функция вычисления производных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3. Градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание функции вычисления частных производных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multivariate_func.partial_x = multivariate_func.partial_y = lambda x: 2 ** x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление координат градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multivariates = multivariate_func.gradient_descent(0.05, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран значения локального минимума:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_local_min(min, multivariates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран трёхмерного графика градиентного спуска функции потерь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d_gradient_chart(x_axis, y_axis, \n",
    "                       multivariate_func.loss_function, \n",
    "                       multivariate_func.start_point,\n",
    "                       multivariates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран анимированного графика градиентного спуска функции потерь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# график содержит анимированные элементы\n",
    "# рекомендуется его запускать локально\n",
    "\n",
    "plot_animated_3d_chart(x_axis, y_axis,\n",
    "                       multivariate_func.loss_function,\n",
    "                       multivariates,\n",
    "                       'Multi-Dimensional Parabola')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Вывод**\n",
    ">\n",
    ">Проведена успешная реализация алгоритма многомерного градиентного спуска и демонстрация его работы на графике."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4. Тестирование на искусственных ландшафтах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тестирование будет проводиться на следующих ландшафтах, образованных сложными функциями: \n",
    "\n",
    "* Функция Матьяса\n",
    "* Функция Изома\n",
    "* Функция Била\n",
    "* Функция Химмельблау"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.4.1. Функция Матьяса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция Матьяса выглядит следующим образом:\n",
    "\n",
    "$$ f(x, y) = 0.26 (x^2 + y^2) - 0.48xy $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определение экземпляра класса `GradientDescent`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matias_func = GradientDescent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение функции потерь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matias_func.loss_function = lambda x, y: 0.26 * (x ** 2 + y ** 2) - 0.48 * x * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран трёхмерного графика, где по осям `X` и `Y` отложены веса исходной модели, а по оси `loss_func(x, y)` - уровень потерь при заданных весах, и точкой `B` в качестве локального минимума:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d_chart(x_axis, y_axis, \n",
    "              matias_func.loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание дифференцируемой функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matias_func.differenciate_function = 0.26 * (x ** 2 + y ** 2) - 0.48 * x * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран частных производных каждой переменной:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matias_func.diff_sympy(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matias_func.diff_sympy(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание функций вычисления частных производных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matias_func.partial_x = lambda x, y: 0.52 * x - 0.48 * y\n",
    "matias_func.partial_y = lambda x, y: -0.48 * x + 0.52 * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание координат начальной точки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matias_func.start_point = (-3, 3, 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление координат градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matiases = matias_func.gradient_descent(0.1, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран значения локального минимума:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_local_min(max, matiases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран трёхмерного графика градиентного спуска функции потерь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d_gradient_chart(x_axis, y_axis, \n",
    "                       matias_func.loss_function, \n",
    "                       matias_func.start_point,\n",
    "                       matiases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран анимированного графика градиентного спуска функции потерь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# график содержит анимированные элементы\n",
    "# рекомендуется его запускать локально\n",
    "\n",
    "plot_animated_3d_chart(x_axis, y_axis,\n",
    "                       matias_func.loss_function,\n",
    "                       matiases,\n",
    "                       'Multi-Dimensional Matias Function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.4.2. Функция Изома"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция Изома выглядит следующим образом:\n",
    "\n",
    "$$ f(x, y) = -cos(x) cos(x) exp \\Bigg(- \\bigg( (x - \\pi)^2 + (y - \\pi)^2 \\bigg) \\Bigg) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определение экземпляра класса `GradientDescent`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "izom_func = GradientDescent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение функции потерь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "izom_func.loss_function = lambda x, y: -np.cos(x) * np.cos(x) * np.exp(-((x - np.pi) ** 2 + (y - np.pi) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран трёхмерного графика, где по осям `X` и `Y` отложены веса исходной модели, а по оси `loss_func(x, y)` - уровень потерь при заданных весах, и точкой `B` в качестве локального минимума:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_3d_chart(x_axis, y_axis, \n",
    "              izom_func.loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание дифференцируемой функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "izom_func.differenciate_function = -cos(x) * cos(x) * exp(-((x - pi) ** 2 + (y - pi) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран частных производных каждой переменной:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "izom_func.diff_sympy(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "izom_func.diff_sympy(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание функций вычисления частных производных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "izom_func.partial_x = lambda x, y: -(-2 * x + 2 * np.pi) * np.exp((-(x - np.pi) ** 2 - (y - np.pi) ** 2)) * \\\n",
    "                                   np.cos(x) ** 2 + 2 * np.exp(-(x - np.pi) ** 2 - (y - np.pi) ** 2) * \\\n",
    "                                   np.sin(x) * np.cos(x)\n",
    "\n",
    "izom_func.partial_y = lambda x, y: -(-2 * y + 2 * np.pi) * np.exp(-(x - np.pi) ** 2 - (y - np.pi) ** 2) * np.cos(x) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание координат начальной точки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "izom_func.start_point = (-1, -1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление координат градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "izoms = izom_func.gradient_descent(1, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран значения локального минимума:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_local_min(max, izoms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран трёхмерного графика градиентного спуска функции потерь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d_gradient_chart(x_axis, y_axis, \n",
    "                       izom_func.loss_function, \n",
    "                       izom_func.start_point,\n",
    "                       izoms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран анимированного графика градиентного спуска функции потерь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# график содержит анимированные элементы\n",
    "# рекомендуется его запускать локально\n",
    "\n",
    "plot_animated_3d_chart(x_axis, y_axis,\n",
    "                       izom_func.loss_function,\n",
    "                       izoms,\n",
    "                       'Multi-Dimensional Izom Function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.4.3. Функция Била"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция Била выглядит следующим образом:\n",
    "\n",
    "$$ f(x, y) = (1.5 - x + xy)^2 + (2.25 - x + xy^2)^2 + (2.625 - x + xy^3)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определение экземпляра класса `GradientDescent`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bil_func = GradientDescent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение функции потерь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bil_func.loss_function = lambda x, y: (1.5 - x + x * y) ** 2 + \\\n",
    "                                      (2.25 - x + x * y ** 2) ** 2 + \\\n",
    "                                      (2.625 - x + x * y ** 3) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран трёхмерного графика, где по осям `X` и `Y` отложены веса исходной модели, а по оси `loss_func(x, y)` - уровень потерь при заданных весах, и точкой `B` в качестве локального минимума:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_3d_chart(x_axis, y_axis, \n",
    "              bil_func.loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание дифференцируемой функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bil_func.differenciate_function = (1.5 - x + x * y) ** 2 + \\\n",
    "                                  (2.25 - x + x * y ** 2) ** 2 + \\\n",
    "                                  (2.625 - x + x * y ** 3) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран частных производных каждой переменной:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bil_func.diff_sympy(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bil_func.diff_sympy(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание функций вычисления частных производных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bil_func.partial_x = lambda x, y: 2.25 * (1.3 * y - 1.3) * (0.6 * x * y - 0.6 * x + 1) + \\\n",
    "                                  5.0625 * (0.8 * y ** 2 - 0.8) * (0.4 * x * y ** 2 - 0.4 * x + 1) + \\\n",
    "                                  6.890625 * (0.762 * y ** 3 - 0.762) * (0.381 * x * y ** 3 - 0.381 * x + 1)\n",
    "\n",
    "bil_func.partial_y = lambda x, y: 15.75 * x * y ** 2 * (0.381 * x * y ** 3 - 0.381 * x + 1) + \\\n",
    "                                  9.0 * x * y * (0.4 * x * y ** 2 - 0.4 * x + 1) + \\\n",
    "                                  3.0 * x * (0.6 * x * y - 0.6 * x + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание координат начальной точки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bil_func.start_point = (-4, 4, 50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление координат градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bils = bil_func.gradient_descent(0.00001, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран значения локального минимума:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_local_min(max, bils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран трёхмерного графика градиентного спуска функции потерь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_3d_gradient_chart(x_axis, y_axis, \n",
    "                       bil_func.loss_function, \n",
    "                       bil_func.start_point,\n",
    "                       bils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран анимированного графика градиентного спуска функции потерь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# график содержит анимированные элементы\n",
    "# рекомендуется его запускать локально\n",
    "\n",
    "plot_animated_3d_chart(x_axis, y_axis,\n",
    "                       bil_func.loss_function,\n",
    "                       bils,\n",
    "                       'Multi-Dimensional Bil Function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.4.4. Функция Химмельблау"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция Химмельбау выглядит следующим образом:\n",
    "\n",
    "$$ f(x, y) = (x^2 + y - 11)^2 + (x + y^2 - 7)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определение экземпляра класса `GradientDescent`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "himmelblau_func = GradientDescent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение функции потерь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "himmelblau_func.loss_function = lambda x, y: (x ** 2 + y - 11) ** 2 + (x + y ** 2 - 7) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран трёхмерного графика, где по осям `X` и `Y` отложены веса исходной модели, а по оси `loss_func(x, y)` - уровень потерь при заданных весах, и точкой `B` в качестве локального минимума:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_3d_chart(x_axis, y_axis, \n",
    "              himmelblau_func.loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание дифференцируемой функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "himmelblau_func.differenciate_function = (x ** 2 + y - 11) ** 2 + (x + y ** 2 - 7) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран частных производных каждой переменной:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "himmelblau_func.diff_sympy(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "himmelblau_func.diff_sympy(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание функций вычисления частных производных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "himmelblau_func.partial_x = lambda x, y: 4 * x * (x ** 2 + y - 11) + 2 * x + 2 * y ** 2 - 14\n",
    "himmelblau_func.partial_y = lambda x, y: 2 * x ** 2 + 4 * y * (x + y ** 2 - 7) + 2 * y - 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание координат начальной точки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "himmelblau_func.start_point = (-1, -1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление координат градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "himmelblaus = himmelblau_func.gradient_descent(0.01, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран значения локального минимума:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_local_min(max, himmelblaus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран трёхмерного графика градиентного спуска функции потерь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_3d_gradient_chart(x_axis, y_axis, \n",
    "                       himmelblau_func.loss_function, \n",
    "                       himmelblau_func.start_point,\n",
    "                       himmelblaus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведение на экран анимированного графика градиентного спуска функции потерь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# график содержит анимированные элементы\n",
    "# рекомендуется его запускать локально\n",
    "\n",
    "plot_animated_3d_chart(x_axis, y_axis,\n",
    "                       himmelblau_func.loss_function,\n",
    "                       himmelblaus,\n",
    "                       'Multi-Dimensional Himmelblaus Function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Вывод**\n",
    ">\n",
    ">Проведено успешное тестирование алгоритма многомерного градиентного спуска на ландшафтах сложных функций."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; opacity: 0.5; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Задача 2: Стохастический градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; opacity: 0.5; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определение экземпляра класса `GradientDescent`:\n",
    "Сохранение функции потерь:\n",
    "Выведение на экран трёхмерного графика, где по осям `X` и `Y` отложены веса исходной модели, а по оси `loss_func(x, y)` - уровень потерь при заданных весах, и точкой `B` в качестве локального минимума:\n",
    "Задание дифференцируемой функции:\n",
    "Выведение на экран частных производных каждой переменной:\n",
    "Задание функции вычисления частных производных:\n",
    "Задание координат начальной точки:\n",
    "Вычисление координат градиентного спуска:\n",
    "Выведение на экран значения локального минимума:\n",
    "Выведение на экран трёхмерного графика градиентного спуска функции потерь:\n",
    "Выведение на экран анимированного графика градиентного спуска функции потерь:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"height: 2px; background-color: blue; opacity: 0.5; margin: 10px 0;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Общий вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size: 20px; padding: 15px 0;\">\n",
    "    <a href=\"#Содержание\" data-toc-modified-id=\"Содержание\" style=\"text-decoration: none; color: #296eaa; border: 2px dashed #296eaa; opacity: 0.8; border-radius: 3px; padding: 10px 80px;\">\n",
    "        Наверх к содержанию ↑\n",
    "    </a>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a2a30c3546c26b196ca13320ca154321d3c7ddf15225431c25ddbdf7ba8fe64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
